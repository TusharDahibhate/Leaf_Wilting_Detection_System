{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DenseNetWithImageNetWeights_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaVAydAUAGO4",
        "colab_type": "code",
        "outputId": "4929ca0b-6b52-499c-c614-7501ae7f5222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nMHXdgEAHwI",
        "colab_type": "code",
        "outputId": "a2d6f23a-406e-470f-c11c-f11fcf6829bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "from keras import backend as K \n",
        "from keras.preprocessing.image import ImageDataGenerator \n",
        "from keras.models import Sequential\n",
        "from keras.models import Model \n",
        "from keras.layers import Conv2D, MaxPooling2D \n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from keras.applications import InceptionResNetV2 \n",
        "! pip install split-folders\n",
        "import tensorflow as tf\n",
        "import split_folders\n",
        "from keras import optimizers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting split-folders\n",
            "  Downloading https://files.pythonhosted.org/packages/20/67/29dda743e6d23ac1ea3d16704d8bbb48d65faf3f1b1eaf53153b3da56c56/split_folders-0.3.1-py3-none-any.whl\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUN8hKackwl2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications import densenet\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras import optimizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foMCZVxjO33n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip /content/drive/My\\ Drive/NN-ProjectC/Version-2/Training/Augmented_Data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5I6nar_AklM",
        "colab_type": "code",
        "outputId": "678565f3-0ea9-40b6-cfa4-5c360df36695",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "training_data_folder = \"/content/Augmented_Data\"\n",
        "split_folders.ratio(training_data_folder, output = \"train_val\", seed=0, ratio = (.7,.3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying files: 12824 files [00:03, 3921.65 files/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOy4heTlB312",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_dir = \"train_val/train\"\n",
        "val_data_dir = \"train_val/val\"\n",
        "num_train = 0\n",
        "num_val = 0\n",
        "\n",
        "for i in os.listdir(train_data_dir):\n",
        "  if i.startswith(\".\"):\n",
        "    os.rmdir(os.path.join(train_data_dir, i))\n",
        "  else:\n",
        "    num_train += len(os.listdir(os.path.join(train_data_dir, i)))\n",
        "  \n",
        "for i in os.listdir(val_data_dir):\n",
        "  if i.startswith(\".\"):\n",
        "    os.rmdir(os.path.join(val_data_dir, i))\n",
        "  else:\n",
        "    num_val += len(os.listdir(os.path.join(val_data_dir, i)))\n",
        "\n",
        "img_width, img_height = 224, 224\n",
        "# Check if the images are RGB and change the channels likewise\n",
        "if K.image_data_format() == 'channels_first':\n",
        "  input_shape= (3, img_width, img_height)\n",
        "else:\n",
        "  input_shape = (img_width, img_height, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPaBR2DTHRWG",
        "colab_type": "code",
        "outputId": "db1d829a-d69f-46ac-9a6e-ca88cd054068",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.applications.densenet import DenseNet121\n",
        "\n",
        "base_model = DenseNet121(input_shape=(224, 224, 3),\n",
        "                                  weights='imagenet',\n",
        "                                  include_top = False,\n",
        "                                  pooling='avg')\n",
        "\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = True\n",
        "\n",
        "x = base_model.output\n",
        "x = Dense(1000)(x)\n",
        "\n",
        "x = Activation('relu')(x)\n",
        "x = Dense(500)(x)\n",
        "\n",
        "x = Activation('relu')(x)\n",
        "predictions = Dense(5, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.Adam(), metrics = ['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "Downloading data from https://github.com/keras-team/keras-applications/releases/download/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29089792/29084464 [==============================] - 1s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPadding2D (None, 230, 230, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1/conv (Conv2D)             (None, 112, 112, 64) 9408        zero_padding2d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv1/bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1/conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1/relu (Activation)         (None, 112, 112, 64) 0           conv1/bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_2 (ZeroPadding2D (None, 114, 114, 64) 0           conv1/relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1 (MaxPooling2D)            (None, 56, 56, 64)   0           zero_padding2d_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 56, 56, 64)   256         pool1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_relu (Activation (None, 56, 56, 64)   0           conv2_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 128)  8192        conv2_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 56, 56, 128)  0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_concat (Concatenat (None, 56, 56, 96)   0           pool1[0][0]                      \n",
            "                                                                 conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_0_bn (BatchNormali (None, 56, 56, 96)   384         conv2_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_0_relu (Activation (None, 56, 56, 96)   0           conv2_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 128)  12288       conv2_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 56, 56, 128)  0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_concat (Concatenat (None, 56, 56, 128)  0           conv2_block1_concat[0][0]        \n",
            "                                                                 conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_0_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_0_relu (Activation (None, 56, 56, 128)  0           conv2_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 128)  16384       conv2_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 56, 56, 128)  0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_concat (Concatenat (None, 56, 56, 160)  0           conv2_block2_concat[0][0]        \n",
            "                                                                 conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_0_bn (BatchNormali (None, 56, 56, 160)  640         conv2_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_0_relu (Activation (None, 56, 56, 160)  0           conv2_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_conv (Conv2D)    (None, 56, 56, 128)  20480       conv2_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_relu (Activation (None, 56, 56, 128)  0           conv2_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_concat (Concatenat (None, 56, 56, 192)  0           conv2_block3_concat[0][0]        \n",
            "                                                                 conv2_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_0_bn (BatchNormali (None, 56, 56, 192)  768         conv2_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_0_relu (Activation (None, 56, 56, 192)  0           conv2_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_conv (Conv2D)    (None, 56, 56, 128)  24576       conv2_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_relu (Activation (None, 56, 56, 128)  0           conv2_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_concat (Concatenat (None, 56, 56, 224)  0           conv2_block4_concat[0][0]        \n",
            "                                                                 conv2_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_0_bn (BatchNormali (None, 56, 56, 224)  896         conv2_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_0_relu (Activation (None, 56, 56, 224)  0           conv2_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_conv (Conv2D)    (None, 56, 56, 128)  28672       conv2_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_relu (Activation (None, 56, 56, 128)  0           conv2_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_concat (Concatenat (None, 56, 56, 256)  0           conv2_block5_concat[0][0]        \n",
            "                                                                 conv2_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "pool2_bn (BatchNormalization)   (None, 56, 56, 256)  1024        conv2_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "pool2_relu (Activation)         (None, 56, 56, 256)  0           pool2_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool2_conv (Conv2D)             (None, 56, 56, 128)  32768       pool2_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool2_pool (AveragePooling2D)   (None, 28, 28, 128)  0           pool2_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 28, 28, 128)  512         pool2_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_relu (Activation (None, 28, 28, 128)  0           conv3_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  16384       conv3_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_concat (Concatenat (None, 28, 28, 160)  0           pool2_pool[0][0]                 \n",
            "                                                                 conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_0_bn (BatchNormali (None, 28, 28, 160)  640         conv3_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_0_relu (Activation (None, 28, 28, 160)  0           conv3_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  20480       conv3_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_concat (Concatenat (None, 28, 28, 192)  0           conv3_block1_concat[0][0]        \n",
            "                                                                 conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_0_bn (BatchNormali (None, 28, 28, 192)  768         conv3_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_0_relu (Activation (None, 28, 28, 192)  0           conv3_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  24576       conv3_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_concat (Concatenat (None, 28, 28, 224)  0           conv3_block2_concat[0][0]        \n",
            "                                                                 conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_0_bn (BatchNormali (None, 28, 28, 224)  896         conv3_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_0_relu (Activation (None, 28, 28, 224)  0           conv3_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  28672       conv3_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_concat (Concatenat (None, 28, 28, 256)  0           conv3_block3_concat[0][0]        \n",
            "                                                                 conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_0_bn (BatchNormali (None, 28, 28, 256)  1024        conv3_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_0_relu (Activation (None, 28, 28, 256)  0           conv3_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_conv (Conv2D)    (None, 28, 28, 128)  32768       conv3_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_relu (Activation (None, 28, 28, 128)  0           conv3_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_concat (Concatenat (None, 28, 28, 288)  0           conv3_block4_concat[0][0]        \n",
            "                                                                 conv3_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_0_bn (BatchNormali (None, 28, 28, 288)  1152        conv3_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_0_relu (Activation (None, 28, 28, 288)  0           conv3_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_conv (Conv2D)    (None, 28, 28, 128)  36864       conv3_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_relu (Activation (None, 28, 28, 128)  0           conv3_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_concat (Concatenat (None, 28, 28, 320)  0           conv3_block5_concat[0][0]        \n",
            "                                                                 conv3_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_0_bn (BatchNormali (None, 28, 28, 320)  1280        conv3_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_0_relu (Activation (None, 28, 28, 320)  0           conv3_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_conv (Conv2D)    (None, 28, 28, 128)  40960       conv3_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_relu (Activation (None, 28, 28, 128)  0           conv3_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_concat (Concatenat (None, 28, 28, 352)  0           conv3_block6_concat[0][0]        \n",
            "                                                                 conv3_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_0_bn (BatchNormali (None, 28, 28, 352)  1408        conv3_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_0_relu (Activation (None, 28, 28, 352)  0           conv3_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_conv (Conv2D)    (None, 28, 28, 128)  45056       conv3_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_relu (Activation (None, 28, 28, 128)  0           conv3_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_concat (Concatenat (None, 28, 28, 384)  0           conv3_block7_concat[0][0]        \n",
            "                                                                 conv3_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_0_bn (BatchNormali (None, 28, 28, 384)  1536        conv3_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_0_relu (Activation (None, 28, 28, 384)  0           conv3_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_conv (Conv2D)    (None, 28, 28, 128)  49152       conv3_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_relu (Activation (None, 28, 28, 128)  0           conv3_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_concat (Concatenat (None, 28, 28, 416)  0           conv3_block8_concat[0][0]        \n",
            "                                                                 conv3_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_0_bn (BatchNormal (None, 28, 28, 416)  1664        conv3_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_0_relu (Activatio (None, 28, 28, 416)  0           conv3_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_conv (Conv2D)   (None, 28, 28, 128)  53248       conv3_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_concat (Concatena (None, 28, 28, 448)  0           conv3_block9_concat[0][0]        \n",
            "                                                                 conv3_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_0_bn (BatchNormal (None, 28, 28, 448)  1792        conv3_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_0_relu (Activatio (None, 28, 28, 448)  0           conv3_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_conv (Conv2D)   (None, 28, 28, 128)  57344       conv3_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_concat (Concatena (None, 28, 28, 480)  0           conv3_block10_concat[0][0]       \n",
            "                                                                 conv3_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_0_bn (BatchNormal (None, 28, 28, 480)  1920        conv3_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_0_relu (Activatio (None, 28, 28, 480)  0           conv3_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_conv (Conv2D)   (None, 28, 28, 128)  61440       conv3_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_concat (Concatena (None, 28, 28, 512)  0           conv3_block11_concat[0][0]       \n",
            "                                                                 conv3_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool3_bn (BatchNormalization)   (None, 28, 28, 512)  2048        conv3_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool3_relu (Activation)         (None, 28, 28, 512)  0           pool3_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool3_conv (Conv2D)             (None, 28, 28, 256)  131072      pool3_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool3_pool (AveragePooling2D)   (None, 14, 14, 256)  0           pool3_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 14, 14, 256)  1024        pool3_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_relu (Activation (None, 14, 14, 256)  0           conv4_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 128)  32768       conv4_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 14, 14, 128)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_concat (Concatenat (None, 14, 14, 288)  0           pool3_pool[0][0]                 \n",
            "                                                                 conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_0_bn (BatchNormali (None, 14, 14, 288)  1152        conv4_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_0_relu (Activation (None, 14, 14, 288)  0           conv4_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 128)  36864       conv4_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 14, 14, 128)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_concat (Concatenat (None, 14, 14, 320)  0           conv4_block1_concat[0][0]        \n",
            "                                                                 conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_0_bn (BatchNormali (None, 14, 14, 320)  1280        conv4_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_0_relu (Activation (None, 14, 14, 320)  0           conv4_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 128)  40960       conv4_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 14, 14, 128)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_concat (Concatenat (None, 14, 14, 352)  0           conv4_block2_concat[0][0]        \n",
            "                                                                 conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_0_bn (BatchNormali (None, 14, 14, 352)  1408        conv4_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_0_relu (Activation (None, 14, 14, 352)  0           conv4_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 128)  45056       conv4_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 14, 14, 128)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_concat (Concatenat (None, 14, 14, 384)  0           conv4_block3_concat[0][0]        \n",
            "                                                                 conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_0_bn (BatchNormali (None, 14, 14, 384)  1536        conv4_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_0_relu (Activation (None, 14, 14, 384)  0           conv4_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 128)  49152       conv4_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 14, 14, 128)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_concat (Concatenat (None, 14, 14, 416)  0           conv4_block4_concat[0][0]        \n",
            "                                                                 conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_0_bn (BatchNormali (None, 14, 14, 416)  1664        conv4_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_0_relu (Activation (None, 14, 14, 416)  0           conv4_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 128)  53248       conv4_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 14, 14, 128)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_concat (Concatenat (None, 14, 14, 448)  0           conv4_block5_concat[0][0]        \n",
            "                                                                 conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_0_bn (BatchNormali (None, 14, 14, 448)  1792        conv4_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_0_relu (Activation (None, 14, 14, 448)  0           conv4_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_conv (Conv2D)    (None, 14, 14, 128)  57344       conv4_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_relu (Activation (None, 14, 14, 128)  0           conv4_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_concat (Concatenat (None, 14, 14, 480)  0           conv4_block6_concat[0][0]        \n",
            "                                                                 conv4_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_0_bn (BatchNormali (None, 14, 14, 480)  1920        conv4_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_0_relu (Activation (None, 14, 14, 480)  0           conv4_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_conv (Conv2D)    (None, 14, 14, 128)  61440       conv4_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_relu (Activation (None, 14, 14, 128)  0           conv4_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_concat (Concatenat (None, 14, 14, 512)  0           conv4_block7_concat[0][0]        \n",
            "                                                                 conv4_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_0_bn (BatchNormali (None, 14, 14, 512)  2048        conv4_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_0_relu (Activation (None, 14, 14, 512)  0           conv4_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_conv (Conv2D)    (None, 14, 14, 128)  65536       conv4_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_relu (Activation (None, 14, 14, 128)  0           conv4_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_concat (Concatenat (None, 14, 14, 544)  0           conv4_block8_concat[0][0]        \n",
            "                                                                 conv4_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_0_bn (BatchNormal (None, 14, 14, 544)  2176        conv4_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_0_relu (Activatio (None, 14, 14, 544)  0           conv4_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_conv (Conv2D)   (None, 14, 14, 128)  69632       conv4_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_concat (Concatena (None, 14, 14, 576)  0           conv4_block9_concat[0][0]        \n",
            "                                                                 conv4_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_0_bn (BatchNormal (None, 14, 14, 576)  2304        conv4_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_0_relu (Activatio (None, 14, 14, 576)  0           conv4_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_conv (Conv2D)   (None, 14, 14, 128)  73728       conv4_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_concat (Concatena (None, 14, 14, 608)  0           conv4_block10_concat[0][0]       \n",
            "                                                                 conv4_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_0_bn (BatchNormal (None, 14, 14, 608)  2432        conv4_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_0_relu (Activatio (None, 14, 14, 608)  0           conv4_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_conv (Conv2D)   (None, 14, 14, 128)  77824       conv4_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_concat (Concatena (None, 14, 14, 640)  0           conv4_block11_concat[0][0]       \n",
            "                                                                 conv4_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_0_bn (BatchNormal (None, 14, 14, 640)  2560        conv4_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_0_relu (Activatio (None, 14, 14, 640)  0           conv4_block13_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_conv (Conv2D)   (None, 14, 14, 128)  81920       conv4_block13_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block13_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block13_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block13_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_concat (Concatena (None, 14, 14, 672)  0           conv4_block12_concat[0][0]       \n",
            "                                                                 conv4_block13_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_0_bn (BatchNormal (None, 14, 14, 672)  2688        conv4_block13_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_0_relu (Activatio (None, 14, 14, 672)  0           conv4_block14_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_conv (Conv2D)   (None, 14, 14, 128)  86016       conv4_block14_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block14_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block14_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block14_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_concat (Concatena (None, 14, 14, 704)  0           conv4_block13_concat[0][0]       \n",
            "                                                                 conv4_block14_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_0_bn (BatchNormal (None, 14, 14, 704)  2816        conv4_block14_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_0_relu (Activatio (None, 14, 14, 704)  0           conv4_block15_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_conv (Conv2D)   (None, 14, 14, 128)  90112       conv4_block15_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block15_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block15_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block15_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_concat (Concatena (None, 14, 14, 736)  0           conv4_block14_concat[0][0]       \n",
            "                                                                 conv4_block15_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_0_bn (BatchNormal (None, 14, 14, 736)  2944        conv4_block15_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_0_relu (Activatio (None, 14, 14, 736)  0           conv4_block16_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_conv (Conv2D)   (None, 14, 14, 128)  94208       conv4_block16_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block16_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block16_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block16_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_concat (Concatena (None, 14, 14, 768)  0           conv4_block15_concat[0][0]       \n",
            "                                                                 conv4_block16_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_0_bn (BatchNormal (None, 14, 14, 768)  3072        conv4_block16_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_0_relu (Activatio (None, 14, 14, 768)  0           conv4_block17_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_conv (Conv2D)   (None, 14, 14, 128)  98304       conv4_block17_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block17_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block17_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block17_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_concat (Concatena (None, 14, 14, 800)  0           conv4_block16_concat[0][0]       \n",
            "                                                                 conv4_block17_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_0_bn (BatchNormal (None, 14, 14, 800)  3200        conv4_block17_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_0_relu (Activatio (None, 14, 14, 800)  0           conv4_block18_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_conv (Conv2D)   (None, 14, 14, 128)  102400      conv4_block18_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block18_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block18_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block18_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_concat (Concatena (None, 14, 14, 832)  0           conv4_block17_concat[0][0]       \n",
            "                                                                 conv4_block18_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_0_bn (BatchNormal (None, 14, 14, 832)  3328        conv4_block18_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_0_relu (Activatio (None, 14, 14, 832)  0           conv4_block19_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_conv (Conv2D)   (None, 14, 14, 128)  106496      conv4_block19_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block19_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block19_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block19_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_concat (Concatena (None, 14, 14, 864)  0           conv4_block18_concat[0][0]       \n",
            "                                                                 conv4_block19_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_0_bn (BatchNormal (None, 14, 14, 864)  3456        conv4_block19_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_0_relu (Activatio (None, 14, 14, 864)  0           conv4_block20_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_conv (Conv2D)   (None, 14, 14, 128)  110592      conv4_block20_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block20_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block20_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block20_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_concat (Concatena (None, 14, 14, 896)  0           conv4_block19_concat[0][0]       \n",
            "                                                                 conv4_block20_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_0_bn (BatchNormal (None, 14, 14, 896)  3584        conv4_block20_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_0_relu (Activatio (None, 14, 14, 896)  0           conv4_block21_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_conv (Conv2D)   (None, 14, 14, 128)  114688      conv4_block21_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block21_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block21_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block21_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_concat (Concatena (None, 14, 14, 928)  0           conv4_block20_concat[0][0]       \n",
            "                                                                 conv4_block21_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_0_bn (BatchNormal (None, 14, 14, 928)  3712        conv4_block21_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_0_relu (Activatio (None, 14, 14, 928)  0           conv4_block22_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_conv (Conv2D)   (None, 14, 14, 128)  118784      conv4_block22_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block22_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block22_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block22_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_concat (Concatena (None, 14, 14, 960)  0           conv4_block21_concat[0][0]       \n",
            "                                                                 conv4_block22_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_0_bn (BatchNormal (None, 14, 14, 960)  3840        conv4_block22_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_0_relu (Activatio (None, 14, 14, 960)  0           conv4_block23_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_conv (Conv2D)   (None, 14, 14, 128)  122880      conv4_block23_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block23_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block23_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block23_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_concat (Concatena (None, 14, 14, 992)  0           conv4_block22_concat[0][0]       \n",
            "                                                                 conv4_block23_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_0_bn (BatchNormal (None, 14, 14, 992)  3968        conv4_block23_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_0_relu (Activatio (None, 14, 14, 992)  0           conv4_block24_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_conv (Conv2D)   (None, 14, 14, 128)  126976      conv4_block24_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block24_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block24_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block24_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_concat (Concatena (None, 14, 14, 1024) 0           conv4_block23_concat[0][0]       \n",
            "                                                                 conv4_block24_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool4_bn (BatchNormalization)   (None, 14, 14, 1024) 4096        conv4_block24_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool4_relu (Activation)         (None, 14, 14, 1024) 0           pool4_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool4_conv (Conv2D)             (None, 14, 14, 512)  524288      pool4_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool4_pool (AveragePooling2D)   (None, 7, 7, 512)    0           pool4_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 7, 7, 512)    2048        pool4_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_relu (Activation (None, 7, 7, 512)    0           conv5_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 128)    65536       conv5_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 7, 7, 128)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_concat (Concatenat (None, 7, 7, 544)    0           pool4_pool[0][0]                 \n",
            "                                                                 conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_0_bn (BatchNormali (None, 7, 7, 544)    2176        conv5_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_0_relu (Activation (None, 7, 7, 544)    0           conv5_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 128)    69632       conv5_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 7, 7, 128)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_concat (Concatenat (None, 7, 7, 576)    0           conv5_block1_concat[0][0]        \n",
            "                                                                 conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_0_bn (BatchNormali (None, 7, 7, 576)    2304        conv5_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_0_relu (Activation (None, 7, 7, 576)    0           conv5_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 128)    73728       conv5_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 7, 7, 128)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_concat (Concatenat (None, 7, 7, 608)    0           conv5_block2_concat[0][0]        \n",
            "                                                                 conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_0_bn (BatchNormali (None, 7, 7, 608)    2432        conv5_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_0_relu (Activation (None, 7, 7, 608)    0           conv5_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_conv (Conv2D)    (None, 7, 7, 128)    77824       conv5_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_relu (Activation (None, 7, 7, 128)    0           conv5_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_concat (Concatenat (None, 7, 7, 640)    0           conv5_block3_concat[0][0]        \n",
            "                                                                 conv5_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_0_bn (BatchNormali (None, 7, 7, 640)    2560        conv5_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_0_relu (Activation (None, 7, 7, 640)    0           conv5_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_conv (Conv2D)    (None, 7, 7, 128)    81920       conv5_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_relu (Activation (None, 7, 7, 128)    0           conv5_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_concat (Concatenat (None, 7, 7, 672)    0           conv5_block4_concat[0][0]        \n",
            "                                                                 conv5_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_0_bn (BatchNormali (None, 7, 7, 672)    2688        conv5_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_0_relu (Activation (None, 7, 7, 672)    0           conv5_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_conv (Conv2D)    (None, 7, 7, 128)    86016       conv5_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_relu (Activation (None, 7, 7, 128)    0           conv5_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_concat (Concatenat (None, 7, 7, 704)    0           conv5_block5_concat[0][0]        \n",
            "                                                                 conv5_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_0_bn (BatchNormali (None, 7, 7, 704)    2816        conv5_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_0_relu (Activation (None, 7, 7, 704)    0           conv5_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_conv (Conv2D)    (None, 7, 7, 128)    90112       conv5_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_relu (Activation (None, 7, 7, 128)    0           conv5_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_concat (Concatenat (None, 7, 7, 736)    0           conv5_block6_concat[0][0]        \n",
            "                                                                 conv5_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_0_bn (BatchNormali (None, 7, 7, 736)    2944        conv5_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_0_relu (Activation (None, 7, 7, 736)    0           conv5_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_conv (Conv2D)    (None, 7, 7, 128)    94208       conv5_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_relu (Activation (None, 7, 7, 128)    0           conv5_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_concat (Concatenat (None, 7, 7, 768)    0           conv5_block7_concat[0][0]        \n",
            "                                                                 conv5_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_0_bn (BatchNormali (None, 7, 7, 768)    3072        conv5_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_0_relu (Activation (None, 7, 7, 768)    0           conv5_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_conv (Conv2D)    (None, 7, 7, 128)    98304       conv5_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_relu (Activation (None, 7, 7, 128)    0           conv5_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_concat (Concatenat (None, 7, 7, 800)    0           conv5_block8_concat[0][0]        \n",
            "                                                                 conv5_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_0_bn (BatchNormal (None, 7, 7, 800)    3200        conv5_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_0_relu (Activatio (None, 7, 7, 800)    0           conv5_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_conv (Conv2D)   (None, 7, 7, 128)    102400      conv5_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_concat (Concatena (None, 7, 7, 832)    0           conv5_block9_concat[0][0]        \n",
            "                                                                 conv5_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_0_bn (BatchNormal (None, 7, 7, 832)    3328        conv5_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_0_relu (Activatio (None, 7, 7, 832)    0           conv5_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_conv (Conv2D)   (None, 7, 7, 128)    106496      conv5_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_concat (Concatena (None, 7, 7, 864)    0           conv5_block10_concat[0][0]       \n",
            "                                                                 conv5_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_0_bn (BatchNormal (None, 7, 7, 864)    3456        conv5_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_0_relu (Activatio (None, 7, 7, 864)    0           conv5_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_conv (Conv2D)   (None, 7, 7, 128)    110592      conv5_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_concat (Concatena (None, 7, 7, 896)    0           conv5_block11_concat[0][0]       \n",
            "                                                                 conv5_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_0_bn (BatchNormal (None, 7, 7, 896)    3584        conv5_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_0_relu (Activatio (None, 7, 7, 896)    0           conv5_block13_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_1_conv (Conv2D)   (None, 7, 7, 128)    114688      conv5_block13_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block13_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block13_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block13_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_concat (Concatena (None, 7, 7, 928)    0           conv5_block12_concat[0][0]       \n",
            "                                                                 conv5_block13_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_0_bn (BatchNormal (None, 7, 7, 928)    3712        conv5_block13_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_0_relu (Activatio (None, 7, 7, 928)    0           conv5_block14_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_1_conv (Conv2D)   (None, 7, 7, 128)    118784      conv5_block14_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block14_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block14_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block14_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_concat (Concatena (None, 7, 7, 960)    0           conv5_block13_concat[0][0]       \n",
            "                                                                 conv5_block14_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_0_bn (BatchNormal (None, 7, 7, 960)    3840        conv5_block14_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_0_relu (Activatio (None, 7, 7, 960)    0           conv5_block15_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_1_conv (Conv2D)   (None, 7, 7, 128)    122880      conv5_block15_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block15_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block15_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block15_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_concat (Concatena (None, 7, 7, 992)    0           conv5_block14_concat[0][0]       \n",
            "                                                                 conv5_block15_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_0_bn (BatchNormal (None, 7, 7, 992)    3968        conv5_block15_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_0_relu (Activatio (None, 7, 7, 992)    0           conv5_block16_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_1_conv (Conv2D)   (None, 7, 7, 128)    126976      conv5_block16_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block16_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block16_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block16_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_concat (Concatena (None, 7, 7, 1024)   0           conv5_block15_concat[0][0]       \n",
            "                                                                 conv5_block16_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "bn (BatchNormalization)         (None, 7, 7, 1024)   4096        conv5_block16_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "relu (Activation)               (None, 7, 7, 1024)   0           bn[0][0]                         \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool (GlobalAveragePooling2 (None, 1024)         0           relu[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1000)         1025000     avg_pool[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 1000)         0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 500)          500500      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 500)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 5)            2505        activation_2[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 8,565,509\n",
            "Trainable params: 8,481,861\n",
            "Non-trainable params: 83,648\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrvdRhIwHiXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
        "\n",
        "def get_callbacks():\n",
        "\n",
        "   path_checkpoint ='checkpoint_keras'  \n",
        "   log_dir='logs'\n",
        "   \n",
        "   callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
        "                                         monitor='val_loss',\n",
        "                                         verbose=1,\n",
        "                                         save_weights_only=False,\n",
        "                                         save_best_only=True,\n",
        "                                         mode='max',\n",
        "                                         period=1)\n",
        "   \n",
        "   callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                                           patience=5,\n",
        "                                           verbose=1)\n",
        "   \n",
        "   callback_tensorboard = TensorBoard(log_dir=log_dir,\n",
        "                                      histogram_freq=0,\n",
        "                                      write_graph=False)\n",
        "   \n",
        "   callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                          factor=0.1,\n",
        "                                          min_lr=1e-4,\n",
        "                                          patience=3,\n",
        "                                          verbose=1)\n",
        "\n",
        "   callbacks = [callback_checkpoint, callback_tensorboard, callback_reduce_lr]\n",
        "\n",
        "   return callbacks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9MzA0VjHvl8",
        "colab_type": "code",
        "outputId": "4000184a-3c98-466c-a541-7fafde520fe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_data_dir, \n",
        "                                                    target_size =(img_width, img_height), \n",
        "                                                    batch_size = batch_size, class_mode = 'categorical')\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory( val_data_dir, \n",
        "                                                         target_size =(img_width, img_height), \n",
        "                                                         batch_size = batch_size, class_mode ='categorical') \n",
        "\n",
        "model.fit_generator(train_generator, \n",
        "                    steps_per_epoch = num_train // batch_size, \n",
        "                    epochs = epochs, \n",
        "                    validation_data = validation_generator, \n",
        "                    validation_steps = num_val// batch_size, \n",
        "                    callbacks = get_callbacks())\n",
        "\n",
        "model.save_weights(\"data_aug.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8974 images belonging to 5 classes.\n",
            "Found 3850 images belonging to 5 classes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1128: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "Epoch 1/100\n",
            "280/280 [==============================] - 319s 1s/step - loss: 0.9116 - acc: 0.6275 - val_loss: 1.2639 - val_acc: 0.5677\n",
            "\n",
            "Epoch 00001: val_loss improved from -inf to 1.26386, saving model to checkpoint_keras\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1265: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n",
            "Epoch 2/100\n",
            "280/280 [==============================] - 284s 1s/step - loss: 0.6610 - acc: 0.7383 - val_loss: 0.7170 - val_acc: 0.6797\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 1.26386\n",
            "Epoch 3/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.5194 - acc: 0.7886 - val_loss: 0.8007 - val_acc: 0.7142\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 1.26386\n",
            "Epoch 4/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.4121 - acc: 0.8423 - val_loss: 0.5762 - val_acc: 0.8004\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.26386\n",
            "Epoch 5/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.3197 - acc: 0.8809 - val_loss: 1.1856 - val_acc: 0.6930\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 1.26386\n",
            "Epoch 6/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.2615 - acc: 0.9043 - val_loss: 3.1810 - val_acc: 0.4937\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.26386 to 3.18098, saving model to checkpoint_keras\n",
            "Epoch 7/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.2119 - acc: 0.9223 - val_loss: 3.4505 - val_acc: 0.4578\n",
            "\n",
            "Epoch 00007: val_loss improved from 3.18098 to 3.45047, saving model to checkpoint_keras\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 8/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0750 - acc: 0.9754 - val_loss: 0.1316 - val_acc: 0.9542\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 3.45047\n",
            "Epoch 9/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0296 - acc: 0.9912 - val_loss: 0.1262 - val_acc: 0.9573\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 3.45047\n",
            "Epoch 10/100\n",
            "280/280 [==============================] - 284s 1s/step - loss: 0.0178 - acc: 0.9943 - val_loss: 0.1033 - val_acc: 0.9670\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 3.45047\n",
            "Epoch 11/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0080 - acc: 0.9981 - val_loss: 0.1364 - val_acc: 0.9644\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 3.45047\n",
            "Epoch 12/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0081 - acc: 0.9978 - val_loss: 0.1296 - val_acc: 0.9636\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 3.45047\n",
            "Epoch 13/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0080 - acc: 0.9972 - val_loss: 0.1440 - val_acc: 0.9646\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 3.45047\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 14/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0055 - acc: 0.9981 - val_loss: 0.1482 - val_acc: 0.9628\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 3.45047\n",
            "Epoch 15/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0057 - acc: 0.9979 - val_loss: 0.1399 - val_acc: 0.9639\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 3.45047\n",
            "Epoch 16/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0089 - acc: 0.9968 - val_loss: 0.1748 - val_acc: 0.9597\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 3.45047\n",
            "Epoch 17/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0109 - acc: 0.9965 - val_loss: 0.1783 - val_acc: 0.9563\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 3.45047\n",
            "Epoch 18/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0053 - acc: 0.9983 - val_loss: 0.1815 - val_acc: 0.9570\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 3.45047\n",
            "Epoch 19/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0106 - acc: 0.9967 - val_loss: 0.1682 - val_acc: 0.9597\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 3.45047\n",
            "Epoch 20/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0064 - acc: 0.9982 - val_loss: 0.1999 - val_acc: 0.9539\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 3.45047\n",
            "Epoch 21/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0037 - acc: 0.9989 - val_loss: 0.1255 - val_acc: 0.9686\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 3.45047\n",
            "Epoch 22/100\n",
            "280/280 [==============================] - 284s 1s/step - loss: 0.0049 - acc: 0.9985 - val_loss: 0.1979 - val_acc: 0.9586\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 3.45047\n",
            "Epoch 23/100\n",
            "280/280 [==============================] - 282s 1s/step - loss: 0.0038 - acc: 0.9988 - val_loss: 0.2192 - val_acc: 0.9508\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 3.45047\n",
            "Epoch 24/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0065 - acc: 0.9977 - val_loss: 0.1855 - val_acc: 0.9639\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 3.45047\n",
            "Epoch 25/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0081 - acc: 0.9973 - val_loss: 0.1867 - val_acc: 0.9594\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 3.45047\n",
            "Epoch 26/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0065 - acc: 0.9978 - val_loss: 0.1866 - val_acc: 0.9586\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 3.45047\n",
            "Epoch 27/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0086 - acc: 0.9968 - val_loss: 0.1500 - val_acc: 0.9644\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 3.45047\n",
            "Epoch 28/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0035 - acc: 0.9987 - val_loss: 0.1566 - val_acc: 0.9652\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 3.45047\n",
            "Epoch 29/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0061 - acc: 0.9979 - val_loss: 0.1799 - val_acc: 0.9620\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 3.45047\n",
            "Epoch 30/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0022 - acc: 0.9992 - val_loss: 0.1354 - val_acc: 0.9680\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 3.45047\n",
            "Epoch 31/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0046 - acc: 0.9983 - val_loss: 0.1490 - val_acc: 0.9667\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 3.45047\n",
            "Epoch 32/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0031 - acc: 0.9985 - val_loss: 0.2008 - val_acc: 0.9639\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 3.45047\n",
            "Epoch 33/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0021 - acc: 0.9991 - val_loss: 0.1641 - val_acc: 0.9707\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 3.45047\n",
            "Epoch 34/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0041 - acc: 0.9988 - val_loss: 0.1943 - val_acc: 0.9623\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 3.45047\n",
            "Epoch 35/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0102 - acc: 0.9973 - val_loss: 0.1819 - val_acc: 0.9620\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 3.45047\n",
            "Epoch 36/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.1756 - val_acc: 0.9691\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 3.45047\n",
            "Epoch 37/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0070 - acc: 0.9969 - val_loss: 0.2240 - val_acc: 0.9584\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 3.45047\n",
            "Epoch 38/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0111 - acc: 0.9965 - val_loss: 0.1528 - val_acc: 0.9688\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 3.45047\n",
            "Epoch 39/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0064 - acc: 0.9975 - val_loss: 0.1524 - val_acc: 0.9667\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 3.45047\n",
            "Epoch 40/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0032 - acc: 0.9991 - val_loss: 0.1540 - val_acc: 0.9657\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 3.45047\n",
            "Epoch 41/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0053 - acc: 0.9991 - val_loss: 0.1261 - val_acc: 0.9730\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 3.45047\n",
            "Epoch 42/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 6.9081e-04 - acc: 0.9999 - val_loss: 0.1345 - val_acc: 0.9715\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 3.45047\n",
            "Epoch 43/100\n",
            "280/280 [==============================] - 284s 1s/step - loss: 5.0617e-04 - acc: 1.0000 - val_loss: 0.1562 - val_acc: 0.9691\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 3.45047\n",
            "Epoch 44/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0085 - acc: 0.9973 - val_loss: 0.1515 - val_acc: 0.9636\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 3.45047\n",
            "Epoch 45/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0070 - acc: 0.9981 - val_loss: 0.1506 - val_acc: 0.9662\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 3.45047\n",
            "Epoch 46/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0051 - acc: 0.9983 - val_loss: 0.1829 - val_acc: 0.9625\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 3.45047\n",
            "Epoch 47/100\n",
            "280/280 [==============================] - 284s 1s/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.1267 - val_acc: 0.9725\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 3.45047\n",
            "Epoch 48/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 4.0733e-04 - acc: 1.0000 - val_loss: 0.1358 - val_acc: 0.9735\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 3.45047\n",
            "Epoch 49/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.1683 - val_acc: 0.9670\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 3.45047\n",
            "Epoch 50/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0018 - acc: 0.9993 - val_loss: 0.1584 - val_acc: 0.9717\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 3.45047\n",
            "Epoch 51/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0162 - acc: 0.9955 - val_loss: 0.2079 - val_acc: 0.9544\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 3.45047\n",
            "Epoch 52/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0025 - acc: 0.9992 - val_loss: 0.1704 - val_acc: 0.9657\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 3.45047\n",
            "Epoch 53/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 8.8948e-04 - acc: 0.9999 - val_loss: 0.1377 - val_acc: 0.9688\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 3.45047\n",
            "Epoch 54/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0064 - acc: 0.9980 - val_loss: 0.1456 - val_acc: 0.9694\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 3.45047\n",
            "Epoch 55/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0068 - acc: 0.9978 - val_loss: 0.1466 - val_acc: 0.9667\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 3.45047\n",
            "Epoch 56/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1262 - val_acc: 0.9725\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 3.45047\n",
            "Epoch 57/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0055 - acc: 0.9983 - val_loss: 0.1722 - val_acc: 0.9636\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 3.45047\n",
            "Epoch 58/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0042 - acc: 0.9987 - val_loss: 0.1232 - val_acc: 0.9712\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 3.45047\n",
            "Epoch 59/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0036 - acc: 0.9989 - val_loss: 0.2209 - val_acc: 0.9563\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 3.45047\n",
            "Epoch 60/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0013 - acc: 0.9994 - val_loss: 0.1388 - val_acc: 0.9688\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 3.45047\n",
            "Epoch 61/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0030 - acc: 0.9991 - val_loss: 0.1634 - val_acc: 0.9657\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 3.45047\n",
            "Epoch 62/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0051 - acc: 0.9981 - val_loss: 0.1732 - val_acc: 0.9686\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 3.45047\n",
            "Epoch 63/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0037 - acc: 0.9990 - val_loss: 0.1707 - val_acc: 0.9673\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 3.45047\n",
            "Epoch 64/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0044 - acc: 0.9988 - val_loss: 0.2084 - val_acc: 0.9607\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 3.45047\n",
            "Epoch 65/100\n",
            "280/280 [==============================] - 282s 1s/step - loss: 0.0057 - acc: 0.9979 - val_loss: 0.1903 - val_acc: 0.9639\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 3.45047\n",
            "Epoch 66/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0059 - acc: 0.9978 - val_loss: 0.1570 - val_acc: 0.9665\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 3.45047\n",
            "Epoch 67/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0018 - acc: 0.9993 - val_loss: 0.1898 - val_acc: 0.9610\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 3.45047\n",
            "Epoch 68/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0026 - acc: 0.9992 - val_loss: 0.1638 - val_acc: 0.9683\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 3.45047\n",
            "Epoch 69/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0046 - acc: 0.9985 - val_loss: 0.1825 - val_acc: 0.9662\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 3.45047\n",
            "Epoch 70/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0021 - acc: 0.9996 - val_loss: 0.1746 - val_acc: 0.9673\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 3.45047\n",
            "Epoch 71/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0028 - acc: 0.9992 - val_loss: 0.1856 - val_acc: 0.9625\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 3.45047\n",
            "Epoch 72/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0028 - acc: 0.9990 - val_loss: 0.1831 - val_acc: 0.9701\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 3.45047\n",
            "Epoch 73/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0075 - acc: 0.9980 - val_loss: 0.2827 - val_acc: 0.9542\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 3.45047\n",
            "Epoch 74/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0029 - acc: 0.9989 - val_loss: 0.2030 - val_acc: 0.9639\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 3.45047\n",
            "Epoch 75/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0017 - acc: 0.9996 - val_loss: 0.2929 - val_acc: 0.9442\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 3.45047\n",
            "Epoch 76/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 8.0898e-04 - acc: 0.9997 - val_loss: 0.2643 - val_acc: 0.9589\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 3.45047\n",
            "Epoch 77/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0059 - acc: 0.9987 - val_loss: 0.2012 - val_acc: 0.9670\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 3.45047\n",
            "Epoch 78/100\n",
            "280/280 [==============================] - 282s 1s/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.1878 - val_acc: 0.9680\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 3.45047\n",
            "Epoch 79/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0072 - acc: 0.9979 - val_loss: 0.2036 - val_acc: 0.9662\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 3.45047\n",
            "Epoch 80/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0032 - acc: 0.9989 - val_loss: 0.2355 - val_acc: 0.9625\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 3.45047\n",
            "Epoch 81/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0031 - acc: 0.9989 - val_loss: 0.1915 - val_acc: 0.9673\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 3.45047\n",
            "Epoch 82/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0022 - acc: 0.9992 - val_loss: 0.2151 - val_acc: 0.9586\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 3.45047\n",
            "Epoch 83/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0034 - acc: 0.9988 - val_loss: 0.1631 - val_acc: 0.9688\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 3.45047\n",
            "Epoch 84/100\n",
            "280/280 [==============================] - 282s 1s/step - loss: 0.0020 - acc: 0.9996 - val_loss: 0.1639 - val_acc: 0.9701\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 3.45047\n",
            "Epoch 85/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0028 - acc: 0.9991 - val_loss: 0.2293 - val_acc: 0.9607\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 3.45047\n",
            "Epoch 86/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0046 - acc: 0.9985 - val_loss: 0.1924 - val_acc: 0.9660\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 3.45047\n",
            "Epoch 87/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0012 - acc: 0.9999 - val_loss: 0.1789 - val_acc: 0.9728\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 3.45047\n",
            "Epoch 88/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 1.8025e-04 - acc: 1.0000 - val_loss: 0.1775 - val_acc: 0.9728\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 3.45047\n",
            "Epoch 89/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0011 - acc: 0.9996 - val_loss: 0.2082 - val_acc: 0.9678\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 3.45047\n",
            "Epoch 90/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0039 - acc: 0.9989 - val_loss: 0.1818 - val_acc: 0.9675\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 3.45047\n",
            "Epoch 91/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0030 - acc: 0.9988 - val_loss: 0.2112 - val_acc: 0.9550\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 3.45047\n",
            "Epoch 92/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0057 - acc: 0.9975 - val_loss: 0.2976 - val_acc: 0.9484\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 3.45047\n",
            "Epoch 93/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0111 - acc: 0.9973 - val_loss: 0.1868 - val_acc: 0.9654\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 3.45047\n",
            "Epoch 94/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1947 - val_acc: 0.9615\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 3.45047\n",
            "Epoch 95/100\n",
            "280/280 [==============================] - 282s 1s/step - loss: 0.0017 - acc: 0.9996 - val_loss: 0.1809 - val_acc: 0.9683\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 3.45047\n",
            "Epoch 96/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0031 - acc: 0.9991 - val_loss: 0.1901 - val_acc: 0.9646\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 3.45047\n",
            "Epoch 97/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.1904 - val_acc: 0.9673\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 3.45047\n",
            "Epoch 98/100\n",
            "280/280 [==============================] - 282s 1s/step - loss: 0.0042 - acc: 0.9989 - val_loss: 0.1998 - val_acc: 0.9615\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 3.45047\n",
            "Epoch 99/100\n",
            "280/280 [==============================] - 283s 1s/step - loss: 0.0057 - acc: 0.9981 - val_loss: 0.2047 - val_acc: 0.9665\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 3.45047\n",
            "Epoch 100/100\n",
            "280/280 [==============================] - 282s 1s/step - loss: 0.0018 - acc: 0.9991 - val_loss: 0.2191 - val_acc: 0.9646\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 3.45047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ceiff1r7IgmG",
        "colab_type": "code",
        "outputId": "f21045a2-5d6c-4e6c-d300-42b562f09b7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "history = model.history\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.savefig(\"Accuracy_curves.png\")\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.savefig(\"Loss_curves.png\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEBhjTIGzQGCMMQXOAoExxhS4/w8RcgtXQsJR9AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LxudrBTvlbD",
        "colab_type": "code",
        "outputId": "220b9a7c-503f-4cdd-df8b-4d5c8dddf05f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy\n",
        "import sklearn.metrics as metrics\n",
        "img_width, img_height = 224, 224\n",
        "batch_size = 8\n",
        "epochs = 100\n",
        "test_set_dir = \"/content/drive/My Drive/NN-ProjectC/Testing\"\n",
        "\n",
        "num_test = len(os.listdir(test_set_dir))\n",
        "\n",
        "print (\"Number of images in test set: \", num_test)\n",
        "\n",
        "#model = get_model(0.0001)\n",
        "\n",
        "#model.load_weights(\"/content/model_new1.h5\")\n",
        "  \n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(test_set_dir, target_size =(img_width, img_height), \n",
        "                                                  batch_size = batch_size, class_mode =None, shuffle = False) \n",
        "\n",
        "test_steps_per_epoch = numpy.math.ceil(test_generator.samples / test_generator.batch_size)\n",
        "\n",
        "predictions = model.predict_generator(test_generator, steps = test_steps_per_epoch)\n",
        "\n",
        "print(\"PREDICTIONS: --->\")\n",
        "print(predictions)\n",
        "\n",
        "predicted_classes = numpy.argmax(predictions, axis=1)\n",
        "\n",
        "print(\"PREDICTED CLASSES: --->\")\n",
        "print (predicted_classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of images in test set:  1\n",
            "Found 129 images belonging to 1 classes.\n",
            "PREDICTIONS: --->\n",
            "[[1.73102971e-02 9.82689738e-01 5.35851875e-12 1.47401934e-11\n",
            "  1.24530716e-13]\n",
            " [7.21543193e-01 2.78456777e-01 5.83955384e-10 5.38700329e-09\n",
            "  2.40853448e-11]\n",
            " [1.79085594e-10 2.41920226e-08 1.28883926e-08 1.00000000e+00\n",
            "  2.18465850e-08]\n",
            " [2.93658572e-15 1.78514166e-16 1.28543264e-14 2.77485876e-14\n",
            "  1.00000000e+00]\n",
            " [2.73425462e-06 9.99990106e-01 1.25241445e-06 5.86489841e-06\n",
            "  3.47989761e-08]\n",
            " [8.03606110e-14 1.06877710e-11 9.71635879e-12 1.00000000e+00\n",
            "  2.57577848e-11]\n",
            " [9.99999762e-01 1.84351535e-07 1.16402513e-11 1.69903785e-10\n",
            "  6.19901359e-15]\n",
            " [2.46388709e-06 1.50779954e-06 2.02871456e-06 9.99668479e-01\n",
            "  3.25543690e-04]\n",
            " [1.28898313e-02 6.82720959e-01 2.95522332e-01 8.70900042e-03\n",
            "  1.57778355e-04]\n",
            " [3.03220956e-14 7.13221986e-15 4.17255613e-13 5.75007706e-13\n",
            "  1.00000000e+00]\n",
            " [4.86598922e-18 3.49012440e-18 1.45758012e-15 1.83923231e-15\n",
            "  1.00000000e+00]\n",
            " [7.02638865e-01 2.97361135e-01 9.19406995e-10 1.83293647e-08\n",
            "  2.10291229e-11]\n",
            " [9.91609871e-01 8.39006342e-03 8.90876422e-08 5.24595745e-09\n",
            "  1.09564031e-10]\n",
            " [5.90062499e-01 1.81790739e-02 3.91505480e-01 2.51874881e-04\n",
            "  1.02799345e-06]\n",
            " [9.99984622e-01 1.44904134e-05 3.25665468e-07 4.86552381e-07\n",
            "  2.25076527e-10]\n",
            " [3.67735565e-01 6.32264495e-01 3.73392761e-10 2.04576018e-08\n",
            "  1.31040864e-11]\n",
            " [9.99895692e-01 1.04306288e-04 1.34488978e-13 9.20463047e-14\n",
            "  2.64924642e-17]\n",
            " [8.22062795e-13 7.56988981e-13 1.04317999e-09 1.04442566e-10\n",
            "  1.00000000e+00]\n",
            " [9.99946475e-01 1.34247357e-05 3.34650504e-05 6.60361275e-06\n",
            "  3.55729379e-09]\n",
            " [9.99993682e-01 6.34028675e-06 3.68393955e-08 1.86525000e-08\n",
            "  1.75810269e-11]\n",
            " [2.69930024e-04 1.90679412e-02 5.96125484e-01 3.58667850e-01\n",
            "  2.58688498e-02]\n",
            " [1.00000000e+00 7.06830590e-13 5.72734243e-13 2.60503865e-14\n",
            "  1.54087059e-20]\n",
            " [1.00000000e+00 4.99084951e-09 2.23930666e-10 2.25533786e-10\n",
            "  5.36644297e-13]\n",
            " [5.51986842e-13 8.54069870e-10 1.00000000e+00 4.14855350e-08\n",
            "  6.65296767e-12]\n",
            " [7.07931349e-07 4.90363673e-05 3.12442644e-05 9.99918461e-01\n",
            "  6.30888962e-07]\n",
            " [1.07674202e-06 1.12278576e-06 9.56623580e-07 9.99730885e-01\n",
            "  2.66049057e-04]\n",
            " [1.84001303e-09 1.47669607e-06 8.67391100e-06 9.99559224e-01\n",
            "  4.30638262e-04]\n",
            " [1.21769118e-15 2.33137299e-17 1.43159802e-15 3.00169863e-15\n",
            "  1.00000000e+00]\n",
            " [9.99996662e-01 3.36352991e-06 3.15107830e-13 1.85000908e-12\n",
            "  7.95926738e-16]\n",
            " [9.99729455e-01 2.57641979e-04 9.42225597e-06 3.45503622e-06\n",
            "  2.50368171e-09]\n",
            " [1.00000000e+00 1.86425406e-13 5.95319446e-15 1.00302190e-15\n",
            "  1.17224151e-20]\n",
            " [3.69963593e-09 1.00000000e+00 8.17802382e-14 1.40449510e-11\n",
            "  2.77292096e-15]\n",
            " [1.29364706e-15 7.39336626e-16 7.80089020e-14 6.95839938e-14\n",
            "  1.00000000e+00]\n",
            " [9.98380780e-01 2.53787930e-05 7.45589614e-06 1.58614304e-03\n",
            "  2.23893181e-07]\n",
            " [1.00000000e+00 1.66009784e-14 4.29468104e-17 4.16610425e-18\n",
            "  9.72788874e-24]\n",
            " [6.19846049e-11 1.06051345e-09 1.00000000e+00 3.10589487e-09\n",
            "  2.33122029e-11]\n",
            " [4.69466497e-04 9.98895407e-01 5.69038093e-04 6.57782148e-05\n",
            "  1.98519658e-07]\n",
            " [1.00000000e+00 3.87839594e-10 3.84583476e-11 1.28230985e-11\n",
            "  2.44228080e-16]\n",
            " [3.80949670e-04 9.96909678e-01 2.53844773e-03 1.62441516e-04\n",
            "  8.46189869e-06]\n",
            " [5.42782899e-03 9.94572222e-01 7.18926377e-11 5.60404945e-10\n",
            "  5.49867140e-12]\n",
            " [5.73888713e-07 6.79765362e-04 9.96194839e-01 3.11456528e-03\n",
            "  1.01771484e-05]\n",
            " [7.03742487e-09 9.17464422e-06 9.99932051e-01 5.70846460e-05\n",
            "  1.66577365e-06]\n",
            " [9.99841690e-01 3.28460883e-05 2.46615718e-06 1.22980389e-04\n",
            "  7.16765758e-09]\n",
            " [9.69607383e-03 9.01358306e-01 1.03075814e-03 8.78263935e-02\n",
            "  8.84638575e-05]\n",
            " [1.81847526e-09 8.93189025e-08 9.99999642e-01 2.91096086e-07\n",
            "  5.31153432e-09]\n",
            " [3.95727991e-15 7.16469764e-15 4.55261751e-13 4.33942568e-13\n",
            "  1.00000000e+00]\n",
            " [1.00000000e+00 2.27346461e-10 3.00476034e-13 9.11240501e-15\n",
            "  1.39347983e-19]\n",
            " [1.00000000e+00 2.51338035e-12 2.65830028e-13 2.93057814e-14\n",
            "  3.80398586e-20]\n",
            " [7.93746556e-04 9.14789923e-03 1.50870681e-02 9.67123389e-01\n",
            "  7.84777198e-03]\n",
            " [2.02164738e-05 9.99977708e-01 9.43482803e-08 1.90175274e-06\n",
            "  1.16742092e-08]\n",
            " [9.99997973e-01 1.94115660e-06 1.56514602e-07 1.29194264e-08\n",
            "  1.03599698e-11]\n",
            " [9.99999285e-01 7.30928036e-07 4.97412023e-09 3.62843394e-10\n",
            "  5.56803030e-13]\n",
            " [1.00000000e+00 1.97962188e-10 1.88829386e-11 7.93269259e-13\n",
            "  1.67068233e-17]\n",
            " [3.03261641e-05 8.41995239e-01 8.74468088e-02 7.04410896e-02\n",
            "  8.65518523e-05]\n",
            " [9.99987721e-01 1.18163507e-05 2.98438209e-07 1.85090443e-07\n",
            "  4.11462892e-10]\n",
            " [9.99998927e-01 9.37269476e-07 5.67885827e-08 2.61971280e-08\n",
            "  2.97873228e-12]\n",
            " [1.81087231e-07 4.01198264e-07 9.72802241e-07 5.63754584e-05\n",
            "  9.99942064e-01]\n",
            " [9.41432595e-01 5.74734099e-02 1.01635850e-03 7.46952937e-05\n",
            "  2.97125962e-06]\n",
            " [7.65495997e-07 1.23276157e-04 9.99871135e-01 4.60491401e-06\n",
            "  1.35878921e-07]\n",
            " [4.94082575e-04 9.86419141e-01 4.28834901e-04 1.26468129e-02\n",
            "  1.10297406e-05]\n",
            " [9.44038689e-01 5.59589304e-02 2.17262755e-06 1.62704055e-07\n",
            "  9.10474007e-09]\n",
            " [1.00000000e+00 6.07407735e-10 5.61848135e-14 8.21167187e-15\n",
            "  1.56266599e-20]\n",
            " [5.49156498e-14 3.01065664e-14 3.77226597e-13 1.44095554e-12\n",
            "  1.00000000e+00]\n",
            " [1.00000000e+00 1.62472003e-09 1.05179046e-10 2.10984511e-11\n",
            "  4.94916904e-16]\n",
            " [1.00000000e+00 8.61570711e-12 9.78415916e-13 8.16186379e-14\n",
            "  4.36217522e-19]\n",
            " [9.99998093e-01 8.14300734e-07 9.45901668e-09 1.01747708e-06\n",
            "  1.99179735e-11]\n",
            " [9.99998808e-01 1.02299452e-06 1.43190249e-07 2.03716226e-08\n",
            "  1.56813521e-11]\n",
            " [4.48204100e-01 5.26446819e-01 2.08413694e-02 4.26685903e-03\n",
            "  2.40854963e-04]\n",
            " [9.99996543e-01 3.51564586e-06 1.00404562e-09 2.52627519e-09\n",
            "  2.49931586e-13]\n",
            " [3.65744272e-05 9.98363316e-01 1.57659315e-03 2.29983816e-05\n",
            "  5.59118916e-07]\n",
            " [6.80997391e-06 6.75185409e-04 9.92755711e-01 6.42608153e-03\n",
            "  1.36186893e-04]\n",
            " [9.99998331e-01 1.69444490e-06 8.19337842e-09 5.52068125e-09\n",
            "  2.46423650e-12]\n",
            " [1.28717138e-05 1.37899158e-04 2.38205757e-05 6.96918845e-01\n",
            "  3.02906513e-01]\n",
            " [9.14593935e-02 1.52393840e-02 5.95686585e-03 8.86897862e-01\n",
            "  4.46528808e-04]\n",
            " [8.93498003e-01 2.11894599e-04 9.82244834e-02 1.12899300e-03\n",
            "  6.93671359e-03]\n",
            " [1.09862827e-04 9.99890089e-01 1.03473541e-08 2.25883738e-08\n",
            "  4.50996712e-10]\n",
            " [9.75914776e-01 2.40787752e-02 5.26734311e-06 1.21408311e-06\n",
            "  1.33380764e-08]\n",
            " [1.00000000e+00 2.94724176e-11 6.30888310e-12 4.68630505e-13\n",
            "  5.76703209e-18]\n",
            " [9.99611318e-01 3.88748536e-04 1.67354151e-12 1.66430389e-12\n",
            "  1.97899063e-15]\n",
            " [1.00000000e+00 2.93535244e-16 5.18219522e-16 3.29079943e-17\n",
            "  7.29856131e-25]\n",
            " [1.01799951e-05 1.87292098e-04 9.97634530e-01 2.14291085e-03\n",
            "  2.51008532e-05]\n",
            " [8.43203783e-01 1.56769484e-01 1.54208792e-05 1.08980712e-05\n",
            "  4.19656061e-07]\n",
            " [1.12383480e-09 2.97292354e-06 1.89445063e-06 9.99994993e-01\n",
            "  1.12769463e-07]\n",
            " [9.99999762e-01 1.87599099e-07 1.24301003e-09 7.48403006e-11\n",
            "  1.07965064e-14]\n",
            " [2.29126922e-08 6.84990994e-07 9.99998927e-01 2.98535525e-07\n",
            "  1.07507674e-08]\n",
            " [1.00000000e+00 6.64973909e-10 1.09738069e-11 7.85847949e-13\n",
            "  3.32706369e-17]\n",
            " [9.94094431e-01 2.77574174e-03 2.93046748e-03 1.98715628e-04\n",
            "  6.97713858e-07]\n",
            " [9.82510626e-01 1.72640327e-02 2.19624577e-04 5.43462511e-06\n",
            "  2.37890220e-07]\n",
            " [1.00000000e+00 1.27359401e-10 6.87632514e-13 6.45816374e-15\n",
            "  1.20079664e-19]\n",
            " [9.36949078e-08 5.92685856e-09 1.59027778e-08 5.57289219e-08\n",
            "  9.99999881e-01]\n",
            " [9.99999881e-01 6.29552801e-08 2.65689498e-10 1.40035162e-11\n",
            "  4.95081440e-15]\n",
            " [1.00000000e+00 5.53561064e-09 1.30275249e-10 5.86805474e-12\n",
            "  1.71119078e-15]\n",
            " [3.69837843e-02 6.09019662e-06 9.62877035e-01 6.30786380e-05\n",
            "  6.99136363e-05]\n",
            " [9.14489550e-15 6.79583131e-15 9.83996738e-13 5.68433321e-13\n",
            "  1.00000000e+00]\n",
            " [4.34491771e-20 8.05488124e-20 2.42363278e-18 1.16105964e-13\n",
            "  1.00000000e+00]\n",
            " [9.99999881e-01 7.75671438e-08 4.49338344e-10 9.84427401e-11\n",
            "  5.26529402e-14]\n",
            " [2.68232458e-09 1.36216244e-07 9.99999762e-01 6.38178790e-08\n",
            "  1.60496438e-09]\n",
            " [9.99432623e-01 5.62470639e-04 1.20039329e-06 3.73714488e-06\n",
            "  4.33132596e-09]\n",
            " [9.99449074e-01 5.50729630e-04 9.30002031e-08 1.05427056e-07\n",
            "  3.65453806e-10]\n",
            " [3.41326668e-05 5.50640398e-05 9.99480188e-01 2.86822760e-04\n",
            "  1.43837300e-04]\n",
            " [2.41362234e-03 2.31215730e-02 8.30439985e-01 1.23609081e-01\n",
            "  2.04157420e-02]\n",
            " [1.00000000e+00 3.65460051e-10 7.97141866e-12 4.59412131e-13\n",
            "  3.99454087e-17]\n",
            " [7.61859992e-05 9.73022401e-01 2.66840812e-02 2.14539774e-04\n",
            "  2.73363867e-06]\n",
            " [2.59337015e-02 9.47889209e-01 2.55430136e-02 5.71489974e-04\n",
            "  6.25975372e-05]\n",
            " [2.86639057e-04 1.32666096e-01 8.65876138e-01 1.12842117e-03\n",
            "  4.27216219e-05]\n",
            " [3.21557643e-08 1.00000000e+00 7.41423201e-10 9.11829190e-09\n",
            "  5.08714208e-12]\n",
            " [9.98901010e-01 1.35141614e-04 9.54061397e-04 9.72675389e-06\n",
            "  1.32229907e-08]\n",
            " [1.00000000e+00 4.44263570e-10 3.01085580e-11 4.43384180e-13\n",
            "  1.10907467e-17]\n",
            " [3.33487253e-13 4.73174966e-14 1.67559792e-12 3.13462927e-12\n",
            "  1.00000000e+00]\n",
            " [4.98974077e-06 3.39331359e-01 6.54572248e-01 6.03370788e-03\n",
            "  5.77233877e-05]\n",
            " [1.00000000e+00 1.77451817e-10 2.68798926e-13 1.24376204e-13\n",
            "  6.46238886e-19]\n",
            " [1.22511992e-04 9.61074173e-01 3.81037593e-02 6.98365970e-04\n",
            "  1.24661847e-06]\n",
            " [8.40615749e-01 1.59059316e-01 2.86147610e-06 3.21968895e-04\n",
            "  5.24011199e-08]\n",
            " [9.99785125e-01 2.02605079e-04 1.20724180e-05 1.48239138e-07\n",
            "  5.00902708e-10]\n",
            " [1.09683075e-10 5.83880055e-09 3.95185538e-08 9.99999881e-01\n",
            "  8.84070630e-08]\n",
            " [1.52436505e-14 1.02137054e-14 1.16148708e-13 2.59072260e-11\n",
            "  1.00000000e+00]\n",
            " [9.93457973e-01 6.54199114e-03 5.01148989e-09 1.27618467e-08\n",
            "  1.57243437e-11]\n",
            " [3.74574171e-07 5.71433455e-04 3.54166143e-02 9.63996172e-01\n",
            "  1.53603032e-05]\n",
            " [1.00000000e+00 3.32493499e-08 4.74556267e-13 1.61770456e-12\n",
            "  7.84046287e-18]\n",
            " [9.99999046e-01 9.63783691e-07 5.36376554e-10 2.98806230e-10\n",
            "  1.01614890e-13]\n",
            " [1.15706433e-04 9.66600418e-01 5.19914611e-04 3.27586681e-02\n",
            "  5.33898765e-06]\n",
            " [9.99999881e-01 1.45293527e-07 6.35851233e-11 2.92506861e-12\n",
            "  5.74477565e-15]\n",
            " [9.03458197e-09 1.11729314e-05 4.04843777e-05 9.99947667e-01\n",
            "  5.62072614e-07]\n",
            " [1.00000000e+00 3.48400988e-08 2.35686165e-10 4.19519974e-11\n",
            "  5.52977879e-15]\n",
            " [9.15216506e-01 8.47834945e-02 1.18328050e-08 1.49999781e-08\n",
            "  1.49113624e-10]\n",
            " [1.00000000e+00 4.84627789e-11 5.57262353e-13 5.39674134e-14\n",
            "  1.15468520e-18]\n",
            " [9.99999762e-01 2.71614141e-07 1.83129575e-12 4.86659671e-12\n",
            "  2.72415537e-16]\n",
            " [3.94551405e-12 6.69425759e-09 1.00000000e+00 4.37862120e-08\n",
            "  2.17445881e-10]\n",
            " [1.34541111e-07 9.99999881e-01 7.51362750e-10 6.90984825e-10\n",
            "  3.60116884e-13]]\n",
            "PREDICTED CLASSES: --->\n",
            "[1 0 3 4 1 3 0 3 1 4 4 0 0 0 0 1 0 4 0 0 2 0 0 2 3 3 3 4 0 0 0 1 4 0 0 2 1\n",
            " 0 1 1 2 2 0 1 2 4 0 0 3 1 0 0 0 1 0 0 4 0 2 1 0 0 4 0 0 0 0 1 0 1 2 0 3 3\n",
            " 0 1 0 0 0 0 2 0 3 0 2 0 0 0 0 4 0 0 2 4 4 0 2 0 0 2 2 0 1 1 2 1 0 0 4 2 0\n",
            " 1 0 0 3 4 0 3 0 0 1 0 3 0 0 0 0 2 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFhL3mK31SrY",
        "colab_type": "code",
        "outputId": "1b28918f-9cc8-4afc-831e-a0c160603df0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_generator.filenames"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Project_C2_Testing/000561.jpg',\n",
              " 'Project_C2_Testing/000747.jpg',\n",
              " 'Project_C2_Testing/000810.jpg',\n",
              " 'Project_C2_Testing/000934.jpg',\n",
              " 'Project_C2_Testing/001133.jpg',\n",
              " 'Project_C2_Testing/001140.jpg',\n",
              " 'Project_C2_Testing/001273.jpg',\n",
              " 'Project_C2_Testing/001506.jpg',\n",
              " 'Project_C2_Testing/001621.jpg',\n",
              " 'Project_C2_Testing/001659.jpg',\n",
              " 'Project_C2_Testing/002185.jpg',\n",
              " 'Project_C2_Testing/002282.jpg',\n",
              " 'Project_C2_Testing/002433.jpg',\n",
              " 'Project_C2_Testing/002532.jpg',\n",
              " 'Project_C2_Testing/002593.jpg',\n",
              " 'Project_C2_Testing/002762.jpg',\n",
              " 'Project_C2_Testing/002764.jpg',\n",
              " 'Project_C2_Testing/002864.jpg',\n",
              " 'Project_C2_Testing/003145.jpg',\n",
              " 'Project_C2_Testing/003386.jpg',\n",
              " 'Project_C2_Testing/003448.jpg',\n",
              " 'Project_C2_Testing/003550.jpg',\n",
              " 'Project_C2_Testing/003825.jpg',\n",
              " 'Project_C2_Testing/003880.jpg',\n",
              " 'Project_C2_Testing/004191.jpg',\n",
              " 'Project_C2_Testing/004539.jpg',\n",
              " 'Project_C2_Testing/004597.jpg',\n",
              " 'Project_C2_Testing/004898.jpg',\n",
              " 'Project_C2_Testing/004973.jpg',\n",
              " 'Project_C2_Testing/005127.jpg',\n",
              " 'Project_C2_Testing/005272.jpg',\n",
              " 'Project_C2_Testing/005592.jpg',\n",
              " 'Project_C2_Testing/005649.jpg',\n",
              " 'Project_C2_Testing/005829.jpg',\n",
              " 'Project_C2_Testing/005981.jpg',\n",
              " 'Project_C2_Testing/006029.jpg',\n",
              " 'Project_C2_Testing/006164.jpg',\n",
              " 'Project_C2_Testing/006253.jpg',\n",
              " 'Project_C2_Testing/006623.jpg',\n",
              " 'Project_C2_Testing/006828.jpg',\n",
              " 'Project_C2_Testing/007129.jpg',\n",
              " 'Project_C2_Testing/007662.jpg',\n",
              " 'Project_C2_Testing/007675.jpg',\n",
              " 'Project_C2_Testing/007749.jpg',\n",
              " 'Project_C2_Testing/007880.jpg',\n",
              " 'Project_C2_Testing/007921.jpg',\n",
              " 'Project_C2_Testing/007963.jpg',\n",
              " 'Project_C2_Testing/008059.jpg',\n",
              " 'Project_C2_Testing/008208.jpg',\n",
              " 'Project_C2_Testing/008487.jpg',\n",
              " 'Project_C2_Testing/008574.jpg',\n",
              " 'Project_C2_Testing/008628.jpg',\n",
              " 'Project_C2_Testing/008705.jpg',\n",
              " 'Project_C2_Testing/008827.jpg',\n",
              " 'Project_C2_Testing/008990.jpg',\n",
              " 'Project_C2_Testing/009135.jpg',\n",
              " 'Project_C2_Testing/009419.jpg',\n",
              " 'Project_C2_Testing/009496.jpg',\n",
              " 'Project_C2_Testing/009653.jpg',\n",
              " 'Project_C2_Testing/009903.jpg',\n",
              " 'Project_C2_Testing/010172.jpg',\n",
              " 'Project_C2_Testing/010516.jpg',\n",
              " 'Project_C2_Testing/010545.jpg',\n",
              " 'Project_C2_Testing/010595.jpg',\n",
              " 'Project_C2_Testing/010711.jpg',\n",
              " 'Project_C2_Testing/010806.jpg',\n",
              " 'Project_C2_Testing/010966.jpg',\n",
              " 'Project_C2_Testing/010999.jpg',\n",
              " 'Project_C2_Testing/011257.jpg',\n",
              " 'Project_C2_Testing/011305.jpg',\n",
              " 'Project_C2_Testing/011495.jpg',\n",
              " 'Project_C2_Testing/011553.jpg',\n",
              " 'Project_C2_Testing/011612.jpg',\n",
              " 'Project_C2_Testing/011674.jpg',\n",
              " 'Project_C2_Testing/012043.jpg',\n",
              " 'Project_C2_Testing/012068.jpg',\n",
              " 'Project_C2_Testing/012148.jpg',\n",
              " 'Project_C2_Testing/012263.jpg',\n",
              " 'Project_C2_Testing/012419.jpg',\n",
              " 'Project_C2_Testing/012543.jpg',\n",
              " 'Project_C2_Testing/012810.jpg',\n",
              " 'Project_C2_Testing/012847.jpg',\n",
              " 'Project_C2_Testing/012893.jpg',\n",
              " 'Project_C2_Testing/012923.jpg',\n",
              " 'Project_C2_Testing/012955.jpg',\n",
              " 'Project_C2_Testing/013010.jpg',\n",
              " 'Project_C2_Testing/013123.jpg',\n",
              " 'Project_C2_Testing/013436.jpg',\n",
              " 'Project_C2_Testing/013540.jpg',\n",
              " 'Project_C2_Testing/013591.jpg',\n",
              " 'Project_C2_Testing/013803.jpg',\n",
              " 'Project_C2_Testing/013818.jpg',\n",
              " 'Project_C2_Testing/013840.jpg',\n",
              " 'Project_C2_Testing/013936.jpg',\n",
              " 'Project_C2_Testing/014047.jpg',\n",
              " 'Project_C2_Testing/014143.jpg',\n",
              " 'Project_C2_Testing/014285.jpg',\n",
              " 'Project_C2_Testing/014614.jpg',\n",
              " 'Project_C2_Testing/014643.jpg',\n",
              " 'Project_C2_Testing/014761.jpg',\n",
              " 'Project_C2_Testing/014943.jpg',\n",
              " 'Project_C2_Testing/015229.jpg',\n",
              " 'Project_C2_Testing/015363.jpg',\n",
              " 'Project_C2_Testing/015503.jpg',\n",
              " 'Project_C2_Testing/015684.jpg',\n",
              " 'Project_C2_Testing/015986.jpg',\n",
              " 'Project_C2_Testing/016015.jpg',\n",
              " 'Project_C2_Testing/016127.jpg',\n",
              " 'Project_C2_Testing/016356.jpg',\n",
              " 'Project_C2_Testing/016682.jpg',\n",
              " 'Project_C2_Testing/016707.jpg',\n",
              " 'Project_C2_Testing/016733.jpg',\n",
              " 'Project_C2_Testing/016995.jpg',\n",
              " 'Project_C2_Testing/017095.jpg',\n",
              " 'Project_C2_Testing/017199.jpg',\n",
              " 'Project_C2_Testing/017213.jpg',\n",
              " 'Project_C2_Testing/017384.jpg',\n",
              " 'Project_C2_Testing/017545.jpg',\n",
              " 'Project_C2_Testing/017777.jpg',\n",
              " 'Project_C2_Testing/017780.jpg',\n",
              " 'Project_C2_Testing/017909.jpg',\n",
              " 'Project_C2_Testing/017923.jpg',\n",
              " 'Project_C2_Testing/017955.jpg',\n",
              " 'Project_C2_Testing/018036.jpg',\n",
              " 'Project_C2_Testing/018658.jpg',\n",
              " 'Project_C2_Testing/018945.jpg',\n",
              " 'Project_C2_Testing/018964.jpg',\n",
              " 'Project_C2_Testing/019159.jpg',\n",
              " 'Project_C2_Testing/019190.jpg']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc6CqQ1S2j1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y_pred = to_categorical(predicted_classes, dtype = 'int32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwZG3R2S8cPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "\n",
        "with open('labels.csv', 'wt') as output:\n",
        "    writer = csv.writer(output)\n",
        "    writer.writerows(y_pred)    \n",
        "output.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiOvx3Q48efY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "f = open('history_final.pckl', 'wb')\n",
        "pickle.dump(history.history, f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Pol4nYFGNiI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}