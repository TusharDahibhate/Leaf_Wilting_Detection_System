{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DenseNet1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaVAydAUAGO4",
        "colab_type": "code",
        "outputId": "a313cb0f-8806-4080-e4c6-369b6a63f2f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nMHXdgEAHwI",
        "colab_type": "code",
        "outputId": "750b6d51-65e8-44b4-a4ce-a76ecdce5d9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "from keras import backend as K \n",
        "from keras.preprocessing.image import ImageDataGenerator \n",
        "from keras.models import Sequential\n",
        "from keras.models import Model \n",
        "from keras.layers import Conv2D, MaxPooling2D \n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from keras.applications import InceptionResNetV2 \n",
        "! pip install split-folders\n",
        "import tensorflow as tf\n",
        "import split_folders\n",
        "from keras import optimizers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting split-folders\n",
            "  Downloading https://files.pythonhosted.org/packages/20/67/29dda743e6d23ac1ea3d16704d8bbb48d65faf3f1b1eaf53153b3da56c56/split_folders-0.3.1-py3-none-any.whl\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foMCZVxjO33n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip /content/drive/My\\ Drive/NN-ProjectC/Version-2/Training/Augmented_Data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5I6nar_AklM",
        "colab_type": "code",
        "outputId": "b4b51c43-f417-4d4f-efd9-f38000d33121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "training_data_folder = \"/content/Augmented_Data\"\n",
        "split_folders.ratio(training_data_folder, output = \"train_val\", seed=0, ratio = (.7,.3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying files: 12824 files [00:02, 5243.96 files/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOy4heTlB312",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_dir = \"train_val/train\"\n",
        "val_data_dir = \"train_val/val\"\n",
        "num_train = 0\n",
        "num_val = 0\n",
        "\n",
        "for i in os.listdir(train_data_dir):\n",
        "  if i.startswith(\".\"):\n",
        "    os.rmdir(os.path.join(train_data_dir, i))\n",
        "  else:\n",
        "    num_train += len(os.listdir(os.path.join(train_data_dir, i)))\n",
        "  \n",
        "for i in os.listdir(val_data_dir):\n",
        "  if i.startswith(\".\"):\n",
        "    os.rmdir(os.path.join(val_data_dir, i))\n",
        "  else:\n",
        "    num_val += len(os.listdir(os.path.join(val_data_dir, i)))\n",
        "\n",
        "img_width, img_height = 224, 224\n",
        "# Check if the images are RGB and change the channels likewise\n",
        "if K.image_data_format() == 'channels_first':\n",
        "  input_shape= (3, img_width, img_height)\n",
        "else:\n",
        "  input_shape = (img_width, img_height, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPaBR2DTHRWG",
        "colab_type": "code",
        "outputId": "761e7443-0820-4e51-cf32-7384b810ee02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.applications.densenet import S\n",
        "from keras.applications.densenet import DenseNet169\n",
        "from keras.applications.densenet import DenseNet201\n",
        "\n",
        "\n",
        "model = DenseNet121(include_top = True, input_shape = (224, 224, 3), weights = None, classes = 5)\n",
        "\n",
        "model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.Adam(0.001), metrics = ['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"densenet121\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_3 (ZeroPadding2D (None, 230, 230, 3)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1/conv (Conv2D)             (None, 112, 112, 64) 9408        zero_padding2d_3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv1/bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1/conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1/relu (Activation)         (None, 112, 112, 64) 0           conv1/bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_4 (ZeroPadding2D (None, 114, 114, 64) 0           conv1/relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1 (MaxPooling2D)            (None, 56, 56, 64)   0           zero_padding2d_4[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 56, 56, 64)   256         pool1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_relu (Activation (None, 56, 56, 64)   0           conv2_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 128)  8192        conv2_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 56, 56, 128)  0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_concat (Concatenat (None, 56, 56, 96)   0           pool1[0][0]                      \n",
            "                                                                 conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_0_bn (BatchNormali (None, 56, 56, 96)   384         conv2_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_0_relu (Activation (None, 56, 56, 96)   0           conv2_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 128)  12288       conv2_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 56, 56, 128)  0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_concat (Concatenat (None, 56, 56, 128)  0           conv2_block1_concat[0][0]        \n",
            "                                                                 conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_0_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_0_relu (Activation (None, 56, 56, 128)  0           conv2_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 128)  16384       conv2_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 56, 56, 128)  0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_concat (Concatenat (None, 56, 56, 160)  0           conv2_block2_concat[0][0]        \n",
            "                                                                 conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_0_bn (BatchNormali (None, 56, 56, 160)  640         conv2_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_0_relu (Activation (None, 56, 56, 160)  0           conv2_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_conv (Conv2D)    (None, 56, 56, 128)  20480       conv2_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_relu (Activation (None, 56, 56, 128)  0           conv2_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_concat (Concatenat (None, 56, 56, 192)  0           conv2_block3_concat[0][0]        \n",
            "                                                                 conv2_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_0_bn (BatchNormali (None, 56, 56, 192)  768         conv2_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_0_relu (Activation (None, 56, 56, 192)  0           conv2_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_conv (Conv2D)    (None, 56, 56, 128)  24576       conv2_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_relu (Activation (None, 56, 56, 128)  0           conv2_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_concat (Concatenat (None, 56, 56, 224)  0           conv2_block4_concat[0][0]        \n",
            "                                                                 conv2_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_0_bn (BatchNormali (None, 56, 56, 224)  896         conv2_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_0_relu (Activation (None, 56, 56, 224)  0           conv2_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_conv (Conv2D)    (None, 56, 56, 128)  28672       conv2_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_relu (Activation (None, 56, 56, 128)  0           conv2_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_concat (Concatenat (None, 56, 56, 256)  0           conv2_block5_concat[0][0]        \n",
            "                                                                 conv2_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "pool2_bn (BatchNormalization)   (None, 56, 56, 256)  1024        conv2_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "pool2_relu (Activation)         (None, 56, 56, 256)  0           pool2_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool2_conv (Conv2D)             (None, 56, 56, 128)  32768       pool2_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool2_pool (AveragePooling2D)   (None, 28, 28, 128)  0           pool2_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 28, 28, 128)  512         pool2_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_relu (Activation (None, 28, 28, 128)  0           conv3_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  16384       conv3_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_concat (Concatenat (None, 28, 28, 160)  0           pool2_pool[0][0]                 \n",
            "                                                                 conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_0_bn (BatchNormali (None, 28, 28, 160)  640         conv3_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_0_relu (Activation (None, 28, 28, 160)  0           conv3_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  20480       conv3_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_concat (Concatenat (None, 28, 28, 192)  0           conv3_block1_concat[0][0]        \n",
            "                                                                 conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_0_bn (BatchNormali (None, 28, 28, 192)  768         conv3_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_0_relu (Activation (None, 28, 28, 192)  0           conv3_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  24576       conv3_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_concat (Concatenat (None, 28, 28, 224)  0           conv3_block2_concat[0][0]        \n",
            "                                                                 conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_0_bn (BatchNormali (None, 28, 28, 224)  896         conv3_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_0_relu (Activation (None, 28, 28, 224)  0           conv3_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  28672       conv3_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_concat (Concatenat (None, 28, 28, 256)  0           conv3_block3_concat[0][0]        \n",
            "                                                                 conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_0_bn (BatchNormali (None, 28, 28, 256)  1024        conv3_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_0_relu (Activation (None, 28, 28, 256)  0           conv3_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_conv (Conv2D)    (None, 28, 28, 128)  32768       conv3_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_relu (Activation (None, 28, 28, 128)  0           conv3_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_concat (Concatenat (None, 28, 28, 288)  0           conv3_block4_concat[0][0]        \n",
            "                                                                 conv3_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_0_bn (BatchNormali (None, 28, 28, 288)  1152        conv3_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_0_relu (Activation (None, 28, 28, 288)  0           conv3_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_conv (Conv2D)    (None, 28, 28, 128)  36864       conv3_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_relu (Activation (None, 28, 28, 128)  0           conv3_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_concat (Concatenat (None, 28, 28, 320)  0           conv3_block5_concat[0][0]        \n",
            "                                                                 conv3_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_0_bn (BatchNormali (None, 28, 28, 320)  1280        conv3_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_0_relu (Activation (None, 28, 28, 320)  0           conv3_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_conv (Conv2D)    (None, 28, 28, 128)  40960       conv3_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_relu (Activation (None, 28, 28, 128)  0           conv3_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_concat (Concatenat (None, 28, 28, 352)  0           conv3_block6_concat[0][0]        \n",
            "                                                                 conv3_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_0_bn (BatchNormali (None, 28, 28, 352)  1408        conv3_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_0_relu (Activation (None, 28, 28, 352)  0           conv3_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_conv (Conv2D)    (None, 28, 28, 128)  45056       conv3_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_relu (Activation (None, 28, 28, 128)  0           conv3_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_concat (Concatenat (None, 28, 28, 384)  0           conv3_block7_concat[0][0]        \n",
            "                                                                 conv3_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_0_bn (BatchNormali (None, 28, 28, 384)  1536        conv3_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_0_relu (Activation (None, 28, 28, 384)  0           conv3_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_conv (Conv2D)    (None, 28, 28, 128)  49152       conv3_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_relu (Activation (None, 28, 28, 128)  0           conv3_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_concat (Concatenat (None, 28, 28, 416)  0           conv3_block8_concat[0][0]        \n",
            "                                                                 conv3_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_0_bn (BatchNormal (None, 28, 28, 416)  1664        conv3_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_0_relu (Activatio (None, 28, 28, 416)  0           conv3_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_conv (Conv2D)   (None, 28, 28, 128)  53248       conv3_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_concat (Concatena (None, 28, 28, 448)  0           conv3_block9_concat[0][0]        \n",
            "                                                                 conv3_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_0_bn (BatchNormal (None, 28, 28, 448)  1792        conv3_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_0_relu (Activatio (None, 28, 28, 448)  0           conv3_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_conv (Conv2D)   (None, 28, 28, 128)  57344       conv3_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_concat (Concatena (None, 28, 28, 480)  0           conv3_block10_concat[0][0]       \n",
            "                                                                 conv3_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_0_bn (BatchNormal (None, 28, 28, 480)  1920        conv3_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_0_relu (Activatio (None, 28, 28, 480)  0           conv3_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_conv (Conv2D)   (None, 28, 28, 128)  61440       conv3_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_concat (Concatena (None, 28, 28, 512)  0           conv3_block11_concat[0][0]       \n",
            "                                                                 conv3_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool3_bn (BatchNormalization)   (None, 28, 28, 512)  2048        conv3_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool3_relu (Activation)         (None, 28, 28, 512)  0           pool3_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool3_conv (Conv2D)             (None, 28, 28, 256)  131072      pool3_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool3_pool (AveragePooling2D)   (None, 14, 14, 256)  0           pool3_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 14, 14, 256)  1024        pool3_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_relu (Activation (None, 14, 14, 256)  0           conv4_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 128)  32768       conv4_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 14, 14, 128)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_concat (Concatenat (None, 14, 14, 288)  0           pool3_pool[0][0]                 \n",
            "                                                                 conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_0_bn (BatchNormali (None, 14, 14, 288)  1152        conv4_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_0_relu (Activation (None, 14, 14, 288)  0           conv4_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 128)  36864       conv4_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 14, 14, 128)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_concat (Concatenat (None, 14, 14, 320)  0           conv4_block1_concat[0][0]        \n",
            "                                                                 conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_0_bn (BatchNormali (None, 14, 14, 320)  1280        conv4_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_0_relu (Activation (None, 14, 14, 320)  0           conv4_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 128)  40960       conv4_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 14, 14, 128)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_concat (Concatenat (None, 14, 14, 352)  0           conv4_block2_concat[0][0]        \n",
            "                                                                 conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_0_bn (BatchNormali (None, 14, 14, 352)  1408        conv4_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_0_relu (Activation (None, 14, 14, 352)  0           conv4_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 128)  45056       conv4_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 14, 14, 128)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_concat (Concatenat (None, 14, 14, 384)  0           conv4_block3_concat[0][0]        \n",
            "                                                                 conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_0_bn (BatchNormali (None, 14, 14, 384)  1536        conv4_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_0_relu (Activation (None, 14, 14, 384)  0           conv4_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 128)  49152       conv4_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 14, 14, 128)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_concat (Concatenat (None, 14, 14, 416)  0           conv4_block4_concat[0][0]        \n",
            "                                                                 conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_0_bn (BatchNormali (None, 14, 14, 416)  1664        conv4_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_0_relu (Activation (None, 14, 14, 416)  0           conv4_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 128)  53248       conv4_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 14, 14, 128)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_concat (Concatenat (None, 14, 14, 448)  0           conv4_block5_concat[0][0]        \n",
            "                                                                 conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_0_bn (BatchNormali (None, 14, 14, 448)  1792        conv4_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_0_relu (Activation (None, 14, 14, 448)  0           conv4_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_conv (Conv2D)    (None, 14, 14, 128)  57344       conv4_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_relu (Activation (None, 14, 14, 128)  0           conv4_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_concat (Concatenat (None, 14, 14, 480)  0           conv4_block6_concat[0][0]        \n",
            "                                                                 conv4_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_0_bn (BatchNormali (None, 14, 14, 480)  1920        conv4_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_0_relu (Activation (None, 14, 14, 480)  0           conv4_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_conv (Conv2D)    (None, 14, 14, 128)  61440       conv4_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_relu (Activation (None, 14, 14, 128)  0           conv4_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_concat (Concatenat (None, 14, 14, 512)  0           conv4_block7_concat[0][0]        \n",
            "                                                                 conv4_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_0_bn (BatchNormali (None, 14, 14, 512)  2048        conv4_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_0_relu (Activation (None, 14, 14, 512)  0           conv4_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_conv (Conv2D)    (None, 14, 14, 128)  65536       conv4_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_relu (Activation (None, 14, 14, 128)  0           conv4_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_concat (Concatenat (None, 14, 14, 544)  0           conv4_block8_concat[0][0]        \n",
            "                                                                 conv4_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_0_bn (BatchNormal (None, 14, 14, 544)  2176        conv4_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_0_relu (Activatio (None, 14, 14, 544)  0           conv4_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_conv (Conv2D)   (None, 14, 14, 128)  69632       conv4_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_concat (Concatena (None, 14, 14, 576)  0           conv4_block9_concat[0][0]        \n",
            "                                                                 conv4_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_0_bn (BatchNormal (None, 14, 14, 576)  2304        conv4_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_0_relu (Activatio (None, 14, 14, 576)  0           conv4_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_conv (Conv2D)   (None, 14, 14, 128)  73728       conv4_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_concat (Concatena (None, 14, 14, 608)  0           conv4_block10_concat[0][0]       \n",
            "                                                                 conv4_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_0_bn (BatchNormal (None, 14, 14, 608)  2432        conv4_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_0_relu (Activatio (None, 14, 14, 608)  0           conv4_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_conv (Conv2D)   (None, 14, 14, 128)  77824       conv4_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_concat (Concatena (None, 14, 14, 640)  0           conv4_block11_concat[0][0]       \n",
            "                                                                 conv4_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_0_bn (BatchNormal (None, 14, 14, 640)  2560        conv4_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_0_relu (Activatio (None, 14, 14, 640)  0           conv4_block13_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_conv (Conv2D)   (None, 14, 14, 128)  81920       conv4_block13_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block13_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block13_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block13_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_concat (Concatena (None, 14, 14, 672)  0           conv4_block12_concat[0][0]       \n",
            "                                                                 conv4_block13_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_0_bn (BatchNormal (None, 14, 14, 672)  2688        conv4_block13_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_0_relu (Activatio (None, 14, 14, 672)  0           conv4_block14_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_conv (Conv2D)   (None, 14, 14, 128)  86016       conv4_block14_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block14_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block14_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block14_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_concat (Concatena (None, 14, 14, 704)  0           conv4_block13_concat[0][0]       \n",
            "                                                                 conv4_block14_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_0_bn (BatchNormal (None, 14, 14, 704)  2816        conv4_block14_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_0_relu (Activatio (None, 14, 14, 704)  0           conv4_block15_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_conv (Conv2D)   (None, 14, 14, 128)  90112       conv4_block15_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block15_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block15_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block15_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_concat (Concatena (None, 14, 14, 736)  0           conv4_block14_concat[0][0]       \n",
            "                                                                 conv4_block15_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_0_bn (BatchNormal (None, 14, 14, 736)  2944        conv4_block15_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_0_relu (Activatio (None, 14, 14, 736)  0           conv4_block16_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_conv (Conv2D)   (None, 14, 14, 128)  94208       conv4_block16_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block16_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block16_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block16_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_concat (Concatena (None, 14, 14, 768)  0           conv4_block15_concat[0][0]       \n",
            "                                                                 conv4_block16_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_0_bn (BatchNormal (None, 14, 14, 768)  3072        conv4_block16_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_0_relu (Activatio (None, 14, 14, 768)  0           conv4_block17_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_conv (Conv2D)   (None, 14, 14, 128)  98304       conv4_block17_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block17_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block17_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block17_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_concat (Concatena (None, 14, 14, 800)  0           conv4_block16_concat[0][0]       \n",
            "                                                                 conv4_block17_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_0_bn (BatchNormal (None, 14, 14, 800)  3200        conv4_block17_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_0_relu (Activatio (None, 14, 14, 800)  0           conv4_block18_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_conv (Conv2D)   (None, 14, 14, 128)  102400      conv4_block18_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block18_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block18_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block18_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_concat (Concatena (None, 14, 14, 832)  0           conv4_block17_concat[0][0]       \n",
            "                                                                 conv4_block18_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_0_bn (BatchNormal (None, 14, 14, 832)  3328        conv4_block18_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_0_relu (Activatio (None, 14, 14, 832)  0           conv4_block19_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_conv (Conv2D)   (None, 14, 14, 128)  106496      conv4_block19_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block19_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block19_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block19_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_concat (Concatena (None, 14, 14, 864)  0           conv4_block18_concat[0][0]       \n",
            "                                                                 conv4_block19_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_0_bn (BatchNormal (None, 14, 14, 864)  3456        conv4_block19_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_0_relu (Activatio (None, 14, 14, 864)  0           conv4_block20_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_conv (Conv2D)   (None, 14, 14, 128)  110592      conv4_block20_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block20_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block20_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block20_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_concat (Concatena (None, 14, 14, 896)  0           conv4_block19_concat[0][0]       \n",
            "                                                                 conv4_block20_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_0_bn (BatchNormal (None, 14, 14, 896)  3584        conv4_block20_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_0_relu (Activatio (None, 14, 14, 896)  0           conv4_block21_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_conv (Conv2D)   (None, 14, 14, 128)  114688      conv4_block21_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block21_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block21_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block21_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_concat (Concatena (None, 14, 14, 928)  0           conv4_block20_concat[0][0]       \n",
            "                                                                 conv4_block21_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_0_bn (BatchNormal (None, 14, 14, 928)  3712        conv4_block21_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_0_relu (Activatio (None, 14, 14, 928)  0           conv4_block22_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_conv (Conv2D)   (None, 14, 14, 128)  118784      conv4_block22_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block22_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block22_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block22_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_concat (Concatena (None, 14, 14, 960)  0           conv4_block21_concat[0][0]       \n",
            "                                                                 conv4_block22_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_0_bn (BatchNormal (None, 14, 14, 960)  3840        conv4_block22_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_0_relu (Activatio (None, 14, 14, 960)  0           conv4_block23_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_conv (Conv2D)   (None, 14, 14, 128)  122880      conv4_block23_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block23_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block23_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block23_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_concat (Concatena (None, 14, 14, 992)  0           conv4_block22_concat[0][0]       \n",
            "                                                                 conv4_block23_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_0_bn (BatchNormal (None, 14, 14, 992)  3968        conv4_block23_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_0_relu (Activatio (None, 14, 14, 992)  0           conv4_block24_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_conv (Conv2D)   (None, 14, 14, 128)  126976      conv4_block24_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block24_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block24_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block24_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_concat (Concatena (None, 14, 14, 1024) 0           conv4_block23_concat[0][0]       \n",
            "                                                                 conv4_block24_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool4_bn (BatchNormalization)   (None, 14, 14, 1024) 4096        conv4_block24_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool4_relu (Activation)         (None, 14, 14, 1024) 0           pool4_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool4_conv (Conv2D)             (None, 14, 14, 512)  524288      pool4_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool4_pool (AveragePooling2D)   (None, 7, 7, 512)    0           pool4_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 7, 7, 512)    2048        pool4_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_relu (Activation (None, 7, 7, 512)    0           conv5_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 128)    65536       conv5_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 7, 7, 128)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_concat (Concatenat (None, 7, 7, 544)    0           pool4_pool[0][0]                 \n",
            "                                                                 conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_0_bn (BatchNormali (None, 7, 7, 544)    2176        conv5_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_0_relu (Activation (None, 7, 7, 544)    0           conv5_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 128)    69632       conv5_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 7, 7, 128)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_concat (Concatenat (None, 7, 7, 576)    0           conv5_block1_concat[0][0]        \n",
            "                                                                 conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_0_bn (BatchNormali (None, 7, 7, 576)    2304        conv5_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_0_relu (Activation (None, 7, 7, 576)    0           conv5_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 128)    73728       conv5_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 7, 7, 128)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_concat (Concatenat (None, 7, 7, 608)    0           conv5_block2_concat[0][0]        \n",
            "                                                                 conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_0_bn (BatchNormali (None, 7, 7, 608)    2432        conv5_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_0_relu (Activation (None, 7, 7, 608)    0           conv5_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_conv (Conv2D)    (None, 7, 7, 128)    77824       conv5_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_relu (Activation (None, 7, 7, 128)    0           conv5_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_concat (Concatenat (None, 7, 7, 640)    0           conv5_block3_concat[0][0]        \n",
            "                                                                 conv5_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_0_bn (BatchNormali (None, 7, 7, 640)    2560        conv5_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_0_relu (Activation (None, 7, 7, 640)    0           conv5_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_conv (Conv2D)    (None, 7, 7, 128)    81920       conv5_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_relu (Activation (None, 7, 7, 128)    0           conv5_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_concat (Concatenat (None, 7, 7, 672)    0           conv5_block4_concat[0][0]        \n",
            "                                                                 conv5_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_0_bn (BatchNormali (None, 7, 7, 672)    2688        conv5_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_0_relu (Activation (None, 7, 7, 672)    0           conv5_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_conv (Conv2D)    (None, 7, 7, 128)    86016       conv5_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_relu (Activation (None, 7, 7, 128)    0           conv5_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_concat (Concatenat (None, 7, 7, 704)    0           conv5_block5_concat[0][0]        \n",
            "                                                                 conv5_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_0_bn (BatchNormali (None, 7, 7, 704)    2816        conv5_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_0_relu (Activation (None, 7, 7, 704)    0           conv5_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_conv (Conv2D)    (None, 7, 7, 128)    90112       conv5_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_relu (Activation (None, 7, 7, 128)    0           conv5_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_concat (Concatenat (None, 7, 7, 736)    0           conv5_block6_concat[0][0]        \n",
            "                                                                 conv5_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_0_bn (BatchNormali (None, 7, 7, 736)    2944        conv5_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_0_relu (Activation (None, 7, 7, 736)    0           conv5_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_conv (Conv2D)    (None, 7, 7, 128)    94208       conv5_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_relu (Activation (None, 7, 7, 128)    0           conv5_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_concat (Concatenat (None, 7, 7, 768)    0           conv5_block7_concat[0][0]        \n",
            "                                                                 conv5_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_0_bn (BatchNormali (None, 7, 7, 768)    3072        conv5_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_0_relu (Activation (None, 7, 7, 768)    0           conv5_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_conv (Conv2D)    (None, 7, 7, 128)    98304       conv5_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_relu (Activation (None, 7, 7, 128)    0           conv5_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_concat (Concatenat (None, 7, 7, 800)    0           conv5_block8_concat[0][0]        \n",
            "                                                                 conv5_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_0_bn (BatchNormal (None, 7, 7, 800)    3200        conv5_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_0_relu (Activatio (None, 7, 7, 800)    0           conv5_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_conv (Conv2D)   (None, 7, 7, 128)    102400      conv5_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_concat (Concatena (None, 7, 7, 832)    0           conv5_block9_concat[0][0]        \n",
            "                                                                 conv5_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_0_bn (BatchNormal (None, 7, 7, 832)    3328        conv5_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_0_relu (Activatio (None, 7, 7, 832)    0           conv5_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_conv (Conv2D)   (None, 7, 7, 128)    106496      conv5_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_concat (Concatena (None, 7, 7, 864)    0           conv5_block10_concat[0][0]       \n",
            "                                                                 conv5_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_0_bn (BatchNormal (None, 7, 7, 864)    3456        conv5_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_0_relu (Activatio (None, 7, 7, 864)    0           conv5_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_conv (Conv2D)   (None, 7, 7, 128)    110592      conv5_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_concat (Concatena (None, 7, 7, 896)    0           conv5_block11_concat[0][0]       \n",
            "                                                                 conv5_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_0_bn (BatchNormal (None, 7, 7, 896)    3584        conv5_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_0_relu (Activatio (None, 7, 7, 896)    0           conv5_block13_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_1_conv (Conv2D)   (None, 7, 7, 128)    114688      conv5_block13_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block13_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block13_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block13_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_concat (Concatena (None, 7, 7, 928)    0           conv5_block12_concat[0][0]       \n",
            "                                                                 conv5_block13_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_0_bn (BatchNormal (None, 7, 7, 928)    3712        conv5_block13_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_0_relu (Activatio (None, 7, 7, 928)    0           conv5_block14_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_1_conv (Conv2D)   (None, 7, 7, 128)    118784      conv5_block14_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block14_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block14_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block14_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_concat (Concatena (None, 7, 7, 960)    0           conv5_block13_concat[0][0]       \n",
            "                                                                 conv5_block14_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_0_bn (BatchNormal (None, 7, 7, 960)    3840        conv5_block14_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_0_relu (Activatio (None, 7, 7, 960)    0           conv5_block15_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_1_conv (Conv2D)   (None, 7, 7, 128)    122880      conv5_block15_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block15_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block15_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block15_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_concat (Concatena (None, 7, 7, 992)    0           conv5_block14_concat[0][0]       \n",
            "                                                                 conv5_block15_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_0_bn (BatchNormal (None, 7, 7, 992)    3968        conv5_block15_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_0_relu (Activatio (None, 7, 7, 992)    0           conv5_block16_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_1_conv (Conv2D)   (None, 7, 7, 128)    126976      conv5_block16_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block16_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block16_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block16_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_concat (Concatena (None, 7, 7, 1024)   0           conv5_block15_concat[0][0]       \n",
            "                                                                 conv5_block16_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "bn (BatchNormalization)         (None, 7, 7, 1024)   4096        conv5_block16_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "relu (Activation)               (None, 7, 7, 1024)   0           bn[0][0]                         \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool (GlobalAveragePooling2 (None, 1024)         0           relu[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "fc1000 (Dense)                  (None, 5)            5125        avg_pool[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 7,042,629\n",
            "Trainable params: 6,958,981\n",
            "Non-trainable params: 83,648\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrvdRhIwHiXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Dfrom keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
        "\n",
        "def get_callbacks():\n",
        "\n",
        "   path_checkpoint ='checkpoint_keras'  \n",
        "   log_dir='logs'\n",
        "   \n",
        "   callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
        "                                         monitor='val_loss',\n",
        "                                         verbose=1,\n",
        "                                         save_weights_only=False,\n",
        "                                         save_best_only=True,\n",
        "                                         mode='max',\n",
        "                                         period=1)\n",
        "   \n",
        "   callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                                           patience=5,\n",
        "                                           verbose=1)\n",
        "   \n",
        "   callback_tensorboard = TensorBoard(log_dir=log_dir,\n",
        "                                      histogram_freq=0,\n",
        "                                      write_graph=False)\n",
        "   \n",
        "   callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                          factor=0.1,\n",
        "                                          min_lr=1e-4,\n",
        "                                          patience=3,\n",
        "                                          verbose=1)\n",
        "\n",
        "   callbacks = [callback_checkpoint, callback_tensorboard, callback_reduce_lr]\n",
        "\n",
        "   return callbacks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9MzA0VjHvl8",
        "colab_type": "code",
        "outputId": "60dbca0b-b860-401a-d3f5-75572adee006",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "weight={0:1,1:3,2:4,3:4,4:6}\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_data_dir, \n",
        "                                                    target_size =(img_width, img_height), \n",
        "                                                    batch_size = batch_size, class_mode = 'categorical')\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory( val_data_dir, \n",
        "                                                         target_size =(img_width, img_height), \n",
        "                                                         batch_size = batch_size, class_mode ='categorical') \n",
        "\n",
        "model.fit_generator(train_generator, \n",
        "                    steps_per_epoch = num_train // batch_size, \n",
        "                    epochs = epochs, \n",
        "                    validation_data = validation_generator, \n",
        "                    validation_steps = num_val// batch_size, \n",
        "                    callbacks = get_callbacks())\n",
        "\n",
        "model.save_weights(\"data_aug.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8974 images belonging to 5 classes.\n",
            "Found 3850 images belonging to 5 classes.\n",
            "Epoch 1/100\n",
            "280/280 [==============================] - 104s 372ms/step - loss: 1.2667 - acc: 0.4611 - val_loss: 2.6013 - val_acc: 0.3755\n",
            "\n",
            "Epoch 00001: val_loss improved from -inf to 2.60130, saving model to checkpoint_keras\n",
            "Epoch 2/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 1.0465 - acc: 0.5465 - val_loss: 5.2098 - val_acc: 0.2635\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.60130 to 5.20981, saving model to checkpoint_keras\n",
            "Epoch 3/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.9848 - acc: 0.5851 - val_loss: 1.5281 - val_acc: 0.4694\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 5.20981\n",
            "Epoch 4/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.9332 - acc: 0.6106 - val_loss: 2.1994 - val_acc: 0.3546\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 5.20981\n",
            "Epoch 5/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.8435 - acc: 0.6480 - val_loss: 1.2516 - val_acc: 0.5285\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 5.20981\n",
            "Epoch 6/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.7880 - acc: 0.6702 - val_loss: 1.4443 - val_acc: 0.5199\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 5.20981\n",
            "Epoch 7/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.7582 - acc: 0.6838 - val_loss: 1.4448 - val_acc: 0.5079\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 5.20981\n",
            "Epoch 8/100\n",
            "280/280 [==============================] - 82s 292ms/step - loss: 0.7047 - acc: 0.7076 - val_loss: 5.5545 - val_acc: 0.2397\n",
            "\n",
            "Epoch 00008: val_loss improved from 5.20981 to 5.55447, saving model to checkpoint_keras\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 9/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.5666 - acc: 0.7784 - val_loss: 0.5850 - val_acc: 0.7645\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 5.55447\n",
            "Epoch 10/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.5140 - acc: 0.7976 - val_loss: 0.6178 - val_acc: 0.7562\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 5.55447\n",
            "Epoch 11/100\n",
            "280/280 [==============================] - 82s 292ms/step - loss: 0.4909 - acc: 0.8034 - val_loss: 0.5922 - val_acc: 0.7520\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 5.55447\n",
            "Epoch 12/100\n",
            "280/280 [==============================] - 81s 290ms/step - loss: 0.4533 - acc: 0.8243 - val_loss: 0.4964 - val_acc: 0.8062\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 5.55447\n",
            "Epoch 13/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.4299 - acc: 0.8302 - val_loss: 0.5475 - val_acc: 0.7800\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 5.55447\n",
            "Epoch 14/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.3996 - acc: 0.8502 - val_loss: 0.5194 - val_acc: 0.7905\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 5.55447\n",
            "Epoch 15/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.3594 - acc: 0.8649 - val_loss: 0.6145 - val_acc: 0.7588\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 5.55447\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 16/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.3344 - acc: 0.8762 - val_loss: 0.6438 - val_acc: 0.7643\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 5.55447\n",
            "Epoch 17/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.2862 - acc: 0.8954 - val_loss: 0.4414 - val_acc: 0.8313\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 5.55447\n",
            "Epoch 18/100\n",
            "280/280 [==============================] - 81s 290ms/step - loss: 0.2471 - acc: 0.9108 - val_loss: 0.5501 - val_acc: 0.8119\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 5.55447\n",
            "Epoch 19/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.2130 - acc: 0.9231 - val_loss: 0.7004 - val_acc: 0.7658\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 5.55447\n",
            "Epoch 20/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.1938 - acc: 0.9322 - val_loss: 0.5530 - val_acc: 0.8057\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 5.55447\n",
            "Epoch 21/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.1434 - acc: 0.9540 - val_loss: 0.4054 - val_acc: 0.8573\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 5.55447\n",
            "Epoch 22/100\n",
            "280/280 [==============================] - 82s 292ms/step - loss: 0.1219 - acc: 0.9593 - val_loss: 0.4394 - val_acc: 0.8428\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 5.55447\n",
            "Epoch 23/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0988 - acc: 0.9720 - val_loss: 0.4381 - val_acc: 0.8604\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 5.55447\n",
            "Epoch 24/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0807 - acc: 0.9747 - val_loss: 0.6234 - val_acc: 0.8133\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 5.55447\n",
            "Epoch 25/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0730 - acc: 0.9778 - val_loss: 0.3535 - val_acc: 0.8824\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 5.55447\n",
            "Epoch 26/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0657 - acc: 0.9797 - val_loss: 0.5142 - val_acc: 0.8557\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 5.55447\n",
            "Epoch 27/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0556 - acc: 0.9826 - val_loss: 0.4415 - val_acc: 0.8604\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 5.55447\n",
            "Epoch 28/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0446 - acc: 0.9875 - val_loss: 0.3730 - val_acc: 0.8753\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 5.55447\n",
            "Epoch 29/100\n",
            "280/280 [==============================] - 81s 290ms/step - loss: 0.0447 - acc: 0.9871 - val_loss: 0.4287 - val_acc: 0.8688\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 5.55447\n",
            "Epoch 30/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0420 - acc: 0.9878 - val_loss: 0.4431 - val_acc: 0.8756\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 5.55447\n",
            "Epoch 31/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0371 - acc: 0.9890 - val_loss: 1.0488 - val_acc: 0.7370\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 5.55447\n",
            "Epoch 32/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0381 - acc: 0.9885 - val_loss: 0.3541 - val_acc: 0.8937\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 5.55447\n",
            "Epoch 33/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0363 - acc: 0.9872 - val_loss: 0.5790 - val_acc: 0.8373\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 5.55447\n",
            "Epoch 34/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0344 - acc: 0.9890 - val_loss: 0.5332 - val_acc: 0.8523\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 5.55447\n",
            "Epoch 35/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0226 - acc: 0.9945 - val_loss: 0.4136 - val_acc: 0.8790\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 5.55447\n",
            "Epoch 36/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0368 - acc: 0.9891 - val_loss: 0.5835 - val_acc: 0.8504\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 5.55447\n",
            "Epoch 37/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0212 - acc: 0.9935 - val_loss: 0.4411 - val_acc: 0.8840\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 5.55447\n",
            "Epoch 38/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0200 - acc: 0.9944 - val_loss: 0.3387 - val_acc: 0.8999\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 5.55447\n",
            "Epoch 39/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0322 - acc: 0.9897 - val_loss: 0.4823 - val_acc: 0.8709\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 5.55447\n",
            "Epoch 40/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0350 - acc: 0.9890 - val_loss: 0.3798 - val_acc: 0.8971\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 5.55447\n",
            "Epoch 41/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0174 - acc: 0.9956 - val_loss: 0.4365 - val_acc: 0.8803\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 5.55447\n",
            "Epoch 42/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0283 - acc: 0.9908 - val_loss: 0.3316 - val_acc: 0.9047\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 5.55447\n",
            "Epoch 43/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0174 - acc: 0.9949 - val_loss: 0.4139 - val_acc: 0.8929\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 5.55447\n",
            "Epoch 44/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0136 - acc: 0.9961 - val_loss: 0.4060 - val_acc: 0.8944\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 5.55447\n",
            "Epoch 45/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0362 - acc: 0.9885 - val_loss: 0.3525 - val_acc: 0.8992\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 5.55447\n",
            "Epoch 46/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0247 - acc: 0.9925 - val_loss: 0.3124 - val_acc: 0.9078\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 5.55447\n",
            "Epoch 47/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0178 - acc: 0.9948 - val_loss: 0.3145 - val_acc: 0.9107\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 5.55447\n",
            "Epoch 48/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0167 - acc: 0.9954 - val_loss: 0.2646 - val_acc: 0.9214\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 5.55447\n",
            "Epoch 49/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0150 - acc: 0.9959 - val_loss: 0.4299 - val_acc: 0.8887\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 5.55447\n",
            "Epoch 50/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0205 - acc: 0.9944 - val_loss: 1.1775 - val_acc: 0.7622\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 5.55447\n",
            "Epoch 51/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0260 - acc: 0.9922 - val_loss: 0.3434 - val_acc: 0.8958\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 5.55447\n",
            "Epoch 52/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0175 - acc: 0.9952 - val_loss: 0.7313 - val_acc: 0.8216\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 5.55447\n",
            "Epoch 53/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0203 - acc: 0.9936 - val_loss: 0.3649 - val_acc: 0.9054\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 5.55447\n",
            "Epoch 54/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0168 - acc: 0.9951 - val_loss: 0.4884 - val_acc: 0.8782\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 5.55447\n",
            "Epoch 55/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0197 - acc: 0.9942 - val_loss: 0.4476 - val_acc: 0.8874\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 5.55447\n",
            "Epoch 56/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0132 - acc: 0.9961 - val_loss: 0.4809 - val_acc: 0.8714\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 5.55447\n",
            "Epoch 57/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0139 - acc: 0.9958 - val_loss: 0.4050 - val_acc: 0.8994\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 5.55447\n",
            "Epoch 58/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0091 - acc: 0.9979 - val_loss: 0.6422 - val_acc: 0.8468\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 5.55447\n",
            "Epoch 59/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0223 - acc: 0.9927 - val_loss: 0.5727 - val_acc: 0.8525\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 5.55447\n",
            "Epoch 60/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0184 - acc: 0.9936 - val_loss: 0.3353 - val_acc: 0.9044\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 5.55447\n",
            "Epoch 61/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0106 - acc: 0.9969 - val_loss: 0.3468 - val_acc: 0.9130\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 5.55447\n",
            "Epoch 62/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0076 - acc: 0.9988 - val_loss: 0.3209 - val_acc: 0.9159\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 5.55447\n",
            "Epoch 63/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0174 - acc: 0.9946 - val_loss: 0.3669 - val_acc: 0.9073\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 5.55447\n",
            "Epoch 64/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0307 - acc: 0.9896 - val_loss: 0.6767 - val_acc: 0.8499\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 5.55447\n",
            "Epoch 65/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0141 - acc: 0.9962 - val_loss: 0.3209 - val_acc: 0.9123\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 5.55447\n",
            "Epoch 66/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0106 - acc: 0.9968 - val_loss: 0.3530 - val_acc: 0.9104\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 5.55447\n",
            "Epoch 67/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0070 - acc: 0.9982 - val_loss: 0.5478 - val_acc: 0.8777\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 5.55447\n",
            "Epoch 68/100\n",
            "280/280 [==============================] - 82s 292ms/step - loss: 0.0094 - acc: 0.9977 - val_loss: 0.3357 - val_acc: 0.9154\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 5.55447\n",
            "Epoch 69/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0085 - acc: 0.9980 - val_loss: 0.4958 - val_acc: 0.8850\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 5.55447\n",
            "Epoch 70/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0222 - acc: 0.9927 - val_loss: 0.4713 - val_acc: 0.8869\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 5.55447\n",
            "Epoch 71/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0116 - acc: 0.9962 - val_loss: 0.5378 - val_acc: 0.8840\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 5.55447\n",
            "Epoch 72/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0128 - acc: 0.9959 - val_loss: 0.5586 - val_acc: 0.8709\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 5.55447\n",
            "Epoch 73/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0116 - acc: 0.9961 - val_loss: 0.4352 - val_acc: 0.9002\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 5.55447\n",
            "Epoch 74/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0190 - acc: 0.9938 - val_loss: 0.3060 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 5.55447\n",
            "Epoch 75/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0108 - acc: 0.9968 - val_loss: 0.3404 - val_acc: 0.9157\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 5.55447\n",
            "Epoch 76/100\n",
            "280/280 [==============================] - 81s 290ms/step - loss: 0.0071 - acc: 0.9977 - val_loss: 0.3184 - val_acc: 0.9212\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 5.55447\n",
            "Epoch 77/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0072 - acc: 0.9980 - val_loss: 0.3207 - val_acc: 0.9183\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 5.55447\n",
            "Epoch 78/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0150 - acc: 0.9948 - val_loss: 0.5528 - val_acc: 0.8672\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 5.55447\n",
            "Epoch 79/100\n",
            "280/280 [==============================] - 82s 292ms/step - loss: 0.0150 - acc: 0.9955 - val_loss: 0.3460 - val_acc: 0.9133\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 5.55447\n",
            "Epoch 80/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0116 - acc: 0.9960 - val_loss: 0.3497 - val_acc: 0.9104\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 5.55447\n",
            "Epoch 81/100\n",
            "280/280 [==============================] - 81s 290ms/step - loss: 0.0085 - acc: 0.9978 - val_loss: 0.3295 - val_acc: 0.9133\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 5.55447\n",
            "Epoch 82/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0055 - acc: 0.9985 - val_loss: 0.3231 - val_acc: 0.9138\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 5.55447\n",
            "Epoch 83/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0082 - acc: 0.9971 - val_loss: 0.8736 - val_acc: 0.8256\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 5.55447\n",
            "Epoch 84/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0159 - acc: 0.9930 - val_loss: 0.3861 - val_acc: 0.8997\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 5.55447\n",
            "Epoch 85/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0173 - acc: 0.9944 - val_loss: 0.4596 - val_acc: 0.8981\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 5.55447\n",
            "Epoch 86/100\n",
            "280/280 [==============================] - 82s 292ms/step - loss: 0.0044 - acc: 0.9990 - val_loss: 0.2861 - val_acc: 0.9269\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 5.55447\n",
            "Epoch 87/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0055 - acc: 0.9983 - val_loss: 0.3752 - val_acc: 0.9070\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 5.55447\n",
            "Epoch 88/100\n",
            "280/280 [==============================] - 81s 290ms/step - loss: 0.0166 - acc: 0.9949 - val_loss: 0.3912 - val_acc: 0.9081\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 5.55447\n",
            "Epoch 89/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0076 - acc: 0.9974 - val_loss: 0.3164 - val_acc: 0.9206\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 5.55447\n",
            "Epoch 90/100\n",
            "280/280 [==============================] - 82s 292ms/step - loss: 0.0068 - acc: 0.9977 - val_loss: 0.6981 - val_acc: 0.8578\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 5.55447\n",
            "Epoch 91/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0150 - acc: 0.9954 - val_loss: 0.4347 - val_acc: 0.8955\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 5.55447\n",
            "Epoch 92/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0114 - acc: 0.9966 - val_loss: 0.4864 - val_acc: 0.8879\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 5.55447\n",
            "Epoch 93/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0056 - acc: 0.9980 - val_loss: 0.3433 - val_acc: 0.9164\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 5.55447\n",
            "Epoch 94/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0045 - acc: 0.9985 - val_loss: 0.3351 - val_acc: 0.9222\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 5.55447\n",
            "Epoch 95/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0113 - acc: 0.9965 - val_loss: 0.5016 - val_acc: 0.8829\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 5.55447\n",
            "Epoch 96/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0161 - acc: 0.9947 - val_loss: 0.3013 - val_acc: 0.9285\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 5.55447\n",
            "Epoch 97/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0047 - acc: 0.9988 - val_loss: 0.3924 - val_acc: 0.9041\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 5.55447\n",
            "Epoch 98/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0049 - acc: 0.9989 - val_loss: 0.4299 - val_acc: 0.9018\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 5.55447\n",
            "Epoch 99/100\n",
            "280/280 [==============================] - 82s 291ms/step - loss: 0.0093 - acc: 0.9972 - val_loss: 0.2735 - val_acc: 0.9374\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 5.55447\n",
            "Epoch 100/100\n",
            "280/280 [==============================] - 81s 291ms/step - loss: 0.0090 - acc: 0.9973 - val_loss: 0.8787 - val_acc: 0.8009\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 5.55447\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ceiff1r7IgmG",
        "colab_type": "code",
        "outputId": "b55492fb-74f6-421a-fce6-8a49da1d0189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "history = model.history\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.savefig(\"Accuracy_curves.png\")\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.savefig(\"Loss_curves.png\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSuU4DQRKKZXjNBAopVSO+38VK9dTzALiCgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LxudrBTvlbD",
        "colab_type": "code",
        "outputId": "b1329cd5-9740-4499-d0c1-deb8d557076c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy\n",
        "import sklearn.metrics as metrics\n",
        "img_width, img_height = 224, 224\n",
        "batch_size = 8\n",
        "epochs = 100\n",
        "test_set_dir = \"/content/drive/My Drive/NN-ProjectC/Testing\"\n",
        "\n",
        "num_test = len(os.listdir(test_set_dir))\n",
        "\n",
        "print (\"Number of images in test set: \", num_test)\n",
        "\n",
        "#model = get_model(0.0001)\n",
        "\n",
        "#model.load_weights(\"/content/model_new1.h5\")\n",
        "  \n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(test_set_dir, target_size =(img_width, img_height), \n",
        "                                                  batch_size = batch_size, class_mode =None, shuffle = False) \n",
        "\n",
        "test_steps_per_epoch = numpy.math.ceil(test_generator.samples / test_generator.batch_size)\n",
        "\n",
        "predictions = model.predict_generator(test_generator, steps = test_steps_per_epoch)\n",
        "\n",
        "print(\"PREDICTIONS: --->\")\n",
        "print(predictions)\n",
        "\n",
        "predicted_classes = numpy.argmax(predictions, axis=1)\n",
        "\n",
        "print(\"PREDICTED CLASSES: --->\")\n",
        "print (predicted_classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of images in test set:  1\n",
            "Found 129 images belonging to 1 classes.\n",
            "PREDICTIONS: --->\n",
            "[[1.34085333e-02 9.86591399e-01 1.89889171e-08 1.24842568e-14\n",
            "  4.17398130e-11]\n",
            " [9.95007277e-01 4.98801749e-03 2.49615005e-06 2.27756550e-06\n",
            "  2.79447443e-09]\n",
            " [2.64708255e-08 1.36256688e-07 4.49107587e-03 9.95454669e-01\n",
            "  5.41240915e-05]\n",
            " [3.84785780e-12 1.80871407e-09 2.68739495e-05 9.96586424e-11\n",
            "  9.99973178e-01]\n",
            " [3.02795513e-07 9.99999046e-01 4.96739672e-09 5.98651467e-12\n",
            "  5.70082648e-07]\n",
            " [1.59024243e-08 3.07715125e-03 3.48082581e-03 8.77105653e-01\n",
            "  1.16336331e-01]\n",
            " [9.98416185e-01 1.57494610e-03 8.78363517e-06 6.01171735e-08\n",
            "  2.37758702e-09]\n",
            " [1.66797149e-11 5.82772830e-10 9.84391874e-11 9.55449923e-06\n",
            "  9.99990463e-01]\n",
            " [5.91334999e-01 3.50023247e-02 3.73617530e-01 4.09878921e-05\n",
            "  4.19326216e-06]\n",
            " [3.56769498e-14 1.04618469e-09 4.80684470e-09 1.57515707e-08\n",
            "  1.00000000e+00]\n",
            " [2.31968316e-15 9.56296587e-12 4.90330866e-12 4.90563042e-11\n",
            "  1.00000000e+00]\n",
            " [6.39925301e-01 3.60074729e-01 1.55507234e-08 1.98187942e-11\n",
            "  5.52192597e-11]\n",
            " [9.99588192e-01 4.11661196e-04 6.05246653e-09 9.34302804e-08\n",
            "  2.19115247e-15]\n",
            " [9.37829375e-01 5.54329939e-02 6.68555219e-03 5.20673493e-05\n",
            "  5.79983617e-09]\n",
            " [9.99821484e-01 1.33436639e-04 4.44726211e-05 5.27295470e-07\n",
            "  2.02494665e-09]\n",
            " [1.87050384e-02 9.81294990e-01 1.37095768e-09 3.36394996e-12\n",
            "  9.00010136e-11]\n",
            " [9.81298983e-01 1.87009312e-02 1.44509585e-07 1.43740075e-10\n",
            "  5.02665435e-15]\n",
            " [3.18822231e-08 8.81225844e-07 9.53609240e-04 1.97789050e-05\n",
            "  9.99025702e-01]\n",
            " [9.99960661e-01 2.76749997e-05 1.13592268e-05 2.91198035e-07\n",
            "  2.70310052e-10]\n",
            " [5.53021908e-01 1.83935754e-03 3.50614712e-02 3.81842911e-01\n",
            "  2.82343179e-02]\n",
            " [1.38103803e-06 1.98420963e-07 1.33409526e-06 9.99996901e-01\n",
            "  2.68951226e-07]\n",
            " [9.99341786e-01 9.27600574e-09 6.26691792e-04 3.15515099e-05\n",
            "  1.01437851e-11]\n",
            " [9.99997139e-01 2.82537394e-06 1.91586830e-10 6.34018671e-10\n",
            "  4.82408872e-13]\n",
            " [1.65943366e-05 9.25121697e-07 1.17998093e-03 9.98802543e-01\n",
            "  1.39857308e-08]\n",
            " [8.90005156e-02 1.24282480e-04 1.20086595e-01 7.90787339e-01\n",
            "  1.25210647e-06]\n",
            " [2.66390856e-08 7.02613245e-10 1.19621169e-09 9.80459332e-01\n",
            "  1.95406172e-02]\n",
            " [9.10944991e-08 2.13814499e-09 8.94770924e-10 9.99624491e-01\n",
            "  3.75366770e-04]\n",
            " [2.86396375e-11 1.21089772e-09 1.42636122e-06 3.48343659e-10\n",
            "  9.99998569e-01]\n",
            " [9.96796191e-01 3.20347049e-03 1.96741876e-08 3.32720845e-07\n",
            "  1.95875996e-10]\n",
            " [7.47237504e-01 7.48994307e-06 2.52106071e-01 6.48914371e-04\n",
            "  5.05776532e-10]\n",
            " [9.99999642e-01 1.51641757e-08 2.70393457e-07 4.32405933e-08\n",
            "  6.96807325e-12]\n",
            " [2.07218598e-03 9.97912943e-01 1.72086189e-07 1.44009960e-09\n",
            "  1.47079891e-05]\n",
            " [4.32767333e-09 5.29464296e-06 1.72886812e-05 1.66045471e-08\n",
            "  9.99977350e-01]\n",
            " [2.76842296e-01 7.83620946e-11 2.62382946e-06 7.23127365e-01\n",
            "  2.77676500e-05]\n",
            " [9.93850827e-01 6.14813576e-03 1.07499511e-06 1.47610111e-08\n",
            "  4.35670380e-12]\n",
            " [2.89381078e-05 5.68347502e-07 9.99687791e-01 2.82568595e-04\n",
            "  1.10468953e-07]\n",
            " [2.10744256e-05 7.33150840e-01 2.66818583e-01 9.79056836e-08\n",
            "  9.42736733e-06]\n",
            " [9.99668717e-01 2.87040282e-04 4.42432065e-05 1.05215858e-09\n",
            "  2.77959882e-12]\n",
            " [2.33683996e-02 9.61199462e-01 1.24729602e-02 2.68483208e-03\n",
            "  2.74355552e-04]\n",
            " [1.76186191e-07 9.99999881e-01 1.02423191e-11 4.45491625e-20\n",
            "  2.66191513e-11]\n",
            " [2.82399036e-04 1.50399821e-04 4.30163704e-02 9.56548512e-01\n",
            "  2.37086670e-06]\n",
            " [3.27974342e-10 7.60110743e-06 9.99991894e-01 1.61555462e-08\n",
            "  4.59073419e-07]\n",
            " [9.88742828e-01 3.06197297e-04 7.65774399e-03 3.29321227e-03\n",
            "  3.94915460e-08]\n",
            " [2.62453496e-01 5.20358793e-03 7.32252419e-01 9.05172856e-05\n",
            "  2.91612938e-08]\n",
            " [5.46649052e-03 8.02080035e-01 1.92453131e-01 2.72503854e-07\n",
            "  2.81429630e-10]\n",
            " [3.97471281e-13 2.77982415e-09 1.75583348e-09 1.38848780e-10\n",
            "  1.00000000e+00]\n",
            " [9.75663781e-01 2.27273858e-06 2.43271161e-02 6.79885716e-06\n",
            "  5.44444047e-12]\n",
            " [9.99892950e-01 3.99627087e-09 1.03676357e-04 3.34214587e-06\n",
            "  2.60331118e-13]\n",
            " [1.49463685e-05 2.61866599e-02 1.28018763e-03 7.63841391e-01\n",
            "  2.08676741e-01]\n",
            " [5.08880021e-07 9.99979138e-01 7.02071645e-09 4.78746816e-12\n",
            "  2.03377112e-05]\n",
            " [9.94379938e-01 5.61734661e-03 9.68980999e-07 7.04155127e-07\n",
            "  1.12094597e-06]\n",
            " [8.57496977e-01 1.40347198e-01 2.15272978e-03 3.12733346e-06\n",
            "  1.89853555e-08]\n",
            " [9.99997377e-01 6.94899194e-10 1.94188124e-06 6.35175752e-07\n",
            "  1.56778379e-14]\n",
            " [3.25620204e-06 8.06705654e-01 1.93221018e-01 1.39113047e-06\n",
            "  6.86771964e-05]\n",
            " [9.99695897e-01 2.78347085e-04 2.09135924e-05 4.85656619e-06\n",
            "  2.50048454e-10]\n",
            " [3.85395527e-01 6.11454844e-01 3.07587674e-03 7.37434966e-05\n",
            "  6.29968824e-08]\n",
            " [4.01931856e-11 1.59498192e-08 3.21640581e-09 1.23385380e-05\n",
            "  9.99987602e-01]\n",
            " [2.37567569e-04 9.99586165e-01 1.52511435e-04 1.10537974e-06\n",
            "  2.26710927e-05]\n",
            " [1.06008010e-05 9.99982238e-01 5.66959716e-06 7.69274618e-13\n",
            "  1.37122242e-06]\n",
            " [8.51588538e-06 9.99619603e-01 4.41510019e-05 1.85147637e-05\n",
            "  3.09282652e-04]\n",
            " [9.99935031e-01 6.43290186e-05 4.04882066e-08 5.82623613e-07\n",
            "  1.14282132e-13]\n",
            " [9.80717778e-01 1.91515945e-02 1.30569140e-04 4.60245220e-09\n",
            "  3.35486916e-11]\n",
            " [3.93738560e-07 1.60782529e-05 4.56028356e-05 9.48164598e-06\n",
            "  9.99928474e-01]\n",
            " [9.99949098e-01 1.67924591e-05 3.40463157e-05 3.06548620e-10\n",
            "  2.64212383e-15]\n",
            " [9.99999642e-01 3.61345030e-07 5.17268504e-08 5.46035439e-10\n",
            "  5.05142061e-17]\n",
            " [9.85238016e-01 1.46129197e-02 3.26714762e-08 6.15728140e-07\n",
            "  1.48396633e-04]\n",
            " [9.99996901e-01 2.76034484e-06 3.23601967e-07 8.58331539e-09\n",
            "  4.94935039e-13]\n",
            " [4.29191828e-01 1.49600453e-07 5.70797741e-01 1.02824988e-05\n",
            "  1.12685891e-12]\n",
            " [1.98753476e-02 9.80120242e-01 4.48428591e-06 3.71096665e-09\n",
            "  1.01701776e-08]\n",
            " [4.81193126e-07 9.99722421e-01 2.77007901e-04 9.40018549e-11\n",
            "  1.61210707e-07]\n",
            " [2.92751502e-05 1.20506505e-04 8.98783147e-01 1.01063140e-01\n",
            "  3.93271102e-06]\n",
            " [4.92635310e-01 4.71718282e-01 3.51751558e-02 4.07616957e-04\n",
            "  6.36176992e-05]\n",
            " [1.67318515e-09 1.57012649e-11 2.22580981e-11 9.99825895e-01\n",
            "  1.74168264e-04]\n",
            " [3.59326065e-01 9.78690665e-03 6.16339922e-01 1.36090238e-02\n",
            "  9.38099751e-04]\n",
            " [8.25662646e-05 1.58181083e-05 9.99383569e-01 2.00487975e-05\n",
            "  4.97957517e-04]\n",
            " [3.11162985e-05 9.99968648e-01 8.96602330e-08 2.61943852e-12\n",
            "  1.74393179e-07]\n",
            " [4.87224430e-01 5.12774467e-01 4.99135865e-07 6.75632293e-07\n",
            "  1.73228432e-10]\n",
            " [9.99738038e-01 5.32759950e-05 2.08704529e-04 1.69942269e-10\n",
            "  4.02997053e-13]\n",
            " [3.11799377e-04 9.99510050e-01 1.78164904e-04 3.28813989e-13\n",
            "  1.10480813e-09]\n",
            " [9.98966217e-01 5.69776694e-06 8.83556553e-04 1.44551639e-04\n",
            "  5.73236001e-08]\n",
            " [2.72060803e-04 2.38405332e-01 7.53912032e-01 1.75095967e-03\n",
            "  5.65959467e-03]\n",
            " [4.05316649e-04 9.99594390e-01 3.08422614e-07 1.93427760e-10\n",
            "  3.05763609e-11]\n",
            " [1.39321673e-06 1.12634234e-06 1.85649424e-05 9.99961019e-01\n",
            "  1.78818318e-05]\n",
            " [9.99766052e-01 1.79452458e-04 5.44981340e-05 8.01861599e-09\n",
            "  1.63693993e-12]\n",
            " [7.81488707e-05 9.99809206e-01 1.12568283e-04 6.93283798e-12\n",
            "  1.05602723e-07]\n",
            " [9.99666810e-01 1.45550832e-04 1.87704412e-04 1.14185418e-10\n",
            "  7.03679194e-13]\n",
            " [1.88156635e-01 3.36131088e-05 8.11449707e-01 3.60002159e-04\n",
            "  8.17709189e-09]\n",
            " [9.99986053e-01 6.06852745e-06 8.29117823e-07 6.98025133e-06\n",
            "  3.80212216e-11]\n",
            " [9.23150361e-01 6.88948855e-02 7.95484334e-03 1.72063661e-08\n",
            "  4.45539612e-12]\n",
            " [9.49029356e-07 9.71868576e-05 3.94960105e-01 3.57792601e-06\n",
            "  6.04938209e-01]\n",
            " [9.47490692e-01 1.15471201e-04 5.23924865e-02 1.37848929e-06\n",
            "  2.92872497e-12]\n",
            " [9.99995828e-01 2.26333015e-07 3.95008374e-06 4.39448078e-09\n",
            "  3.29578773e-16]\n",
            " [5.35403262e-04 4.53401328e-04 9.98999655e-01 8.83737755e-07\n",
            "  1.07449687e-05]\n",
            " [1.25434382e-14 6.62291099e-12 7.28976463e-11 1.37838361e-11\n",
            "  1.00000000e+00]\n",
            " [1.46398119e-11 3.40525420e-11 1.68005008e-05 1.88609029e-05\n",
            "  9.99964356e-01]\n",
            " [9.99339640e-01 6.32370764e-04 1.19536789e-05 1.04493884e-05\n",
            "  5.56869281e-06]\n",
            " [8.77007842e-03 9.45536435e-01 4.56934795e-02 2.29356889e-08\n",
            "  1.73080683e-09]\n",
            " [9.87621963e-01 1.23758400e-02 5.13925658e-09 2.18077162e-06\n",
            "  5.62634303e-13]\n",
            " [9.99609530e-01 3.90552887e-04 3.04351144e-09 5.93175620e-10\n",
            "  2.27138173e-13]\n",
            " [9.41153802e-03 2.86843325e-03 9.26605940e-01 4.21916768e-02\n",
            "  1.89224724e-02]\n",
            " [9.99456085e-03 3.83008068e-04 1.10939356e-04 9.89189386e-01\n",
            "  3.22047010e-04]\n",
            " [8.81760776e-01 1.14857726e-01 3.38154449e-03 1.64824354e-09\n",
            "  1.19801851e-11]\n",
            " [1.92183161e-05 3.53669450e-02 9.63712335e-01 8.87572474e-04\n",
            "  1.39189342e-05]\n",
            " [3.80379446e-02 9.61921453e-01 3.29580907e-05 7.51982043e-06\n",
            "  7.80570417e-08]\n",
            " [8.55877591e-10 1.55412361e-01 8.44061553e-01 4.75401999e-13\n",
            "  5.26091491e-04]\n",
            " [1.07014635e-07 9.99998450e-01 1.49381951e-09 1.06614435e-12\n",
            "  1.43570537e-06]\n",
            " [9.17470098e-01 3.65455235e-05 8.24875459e-02 5.82199982e-06\n",
            "  2.71785455e-10]\n",
            " [9.99997377e-01 6.58898447e-09 2.61945934e-06 5.34788711e-08\n",
            "  1.71225135e-14]\n",
            " [6.98508905e-14 1.79281545e-09 1.80843620e-06 2.10655928e-12\n",
            "  9.99998212e-01]\n",
            " [3.72088887e-10 2.76659284e-05 9.99972343e-01 2.09193149e-11\n",
            "  4.46344508e-08]\n",
            " [9.99313831e-01 3.23390937e-04 1.06684350e-09 3.62691033e-04\n",
            "  4.71092498e-09]\n",
            " [1.02876947e-05 2.89697619e-03 9.97091532e-01 6.54537359e-07\n",
            "  4.86678573e-07]\n",
            " [1.41893979e-02 9.85786676e-01 2.39119327e-05 1.17285573e-07\n",
            "  4.08346607e-10]\n",
            " [9.99706447e-01 1.44171485e-04 1.49217129e-04 1.15072453e-07\n",
            "  9.49723893e-12]\n",
            " [3.45691054e-12 4.46600223e-09 1.12723552e-04 9.97308731e-01\n",
            "  2.57853209e-03]\n",
            " [4.36050126e-16 3.74923676e-15 1.36758793e-09 1.86867233e-09\n",
            "  1.00000000e+00]\n",
            " [9.23694253e-01 7.62991011e-02 6.60294018e-06 1.37210110e-08\n",
            "  9.21654138e-13]\n",
            " [1.50133737e-05 4.32016861e-07 9.66388762e-01 3.35951149e-02\n",
            "  7.31582759e-07]\n",
            " [9.99888062e-01 1.11974659e-04 1.92602490e-09 4.80514251e-09\n",
            "  1.07481396e-12]\n",
            " [2.45080287e-07 9.99999762e-01 1.19816960e-12 4.88212883e-17\n",
            "  4.48497406e-14]\n",
            " [1.93998044e-08 9.99992728e-01 1.77942479e-08 4.90304910e-06\n",
            "  2.35457810e-06]\n",
            " [9.99999762e-01 2.26040086e-07 1.83613819e-10 4.30243796e-09\n",
            "  2.48067693e-14]\n",
            " [3.53413632e-10 2.49597694e-08 3.12733550e-06 9.99996185e-01\n",
            "  6.86350575e-07]\n",
            " [9.99958754e-01 2.29953839e-05 1.82802341e-05 3.56747201e-11\n",
            "  6.36842992e-16]\n",
            " [7.07616329e-01 2.92350858e-01 3.27915477e-05 5.44355849e-09\n",
            "  3.89960508e-10]\n",
            " [9.99929309e-01 1.65843412e-05 4.36837108e-05 3.68373139e-06\n",
            "  6.82922973e-06]\n",
            " [9.99387145e-01 5.93552133e-04 9.10561994e-06 3.86714902e-07\n",
            "  9.82767142e-06]\n",
            " [5.01000941e-05 1.32608255e-02 9.86614764e-01 7.32738190e-05\n",
            "  1.09777466e-06]\n",
            " [1.21585466e-03 9.98688519e-01 9.55692740e-05 1.94801315e-13\n",
            "  7.74305533e-12]]\n",
            "PREDICTED CLASSES: --->\n",
            "[1 0 3 4 1 3 0 4 0 4 4 0 0 0 0 1 0 4 0 0 3 0 0 3 3 3 3 4 0 0 0 1 4 3 0 2 1\n",
            " 0 1 1 3 2 0 2 1 4 0 0 3 1 0 0 0 1 0 1 4 1 1 1 0 0 4 0 0 0 0 2 1 1 2 0 3 2\n",
            " 2 1 1 0 1 0 2 1 3 0 1 0 2 0 0 4 0 0 2 4 4 0 1 0 0 2 3 0 2 1 2 1 0 0 4 2 0\n",
            " 2 1 0 3 4 0 2 0 1 1 0 3 0 0 0 0 2 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFhL3mK31SrY",
        "colab_type": "code",
        "outputId": "7b2f22f6-2942-49a2-a540-c759d95301e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_generator.filenames"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Project_C2_Testing/000561.jpg',\n",
              " 'Project_C2_Testing/000747.jpg',\n",
              " 'Project_C2_Testing/000810.jpg',\n",
              " 'Project_C2_Testing/000934.jpg',\n",
              " 'Project_C2_Testing/001133.jpg',\n",
              " 'Project_C2_Testing/001140.jpg',\n",
              " 'Project_C2_Testing/001273.jpg',\n",
              " 'Project_C2_Testing/001506.jpg',\n",
              " 'Project_C2_Testing/001621.jpg',\n",
              " 'Project_C2_Testing/001659.jpg',\n",
              " 'Project_C2_Testing/002185.jpg',\n",
              " 'Project_C2_Testing/002282.jpg',\n",
              " 'Project_C2_Testing/002433.jpg',\n",
              " 'Project_C2_Testing/002532.jpg',\n",
              " 'Project_C2_Testing/002593.jpg',\n",
              " 'Project_C2_Testing/002762.jpg',\n",
              " 'Project_C2_Testing/002764.jpg',\n",
              " 'Project_C2_Testing/002864.jpg',\n",
              " 'Project_C2_Testing/003145.jpg',\n",
              " 'Project_C2_Testing/003386.jpg',\n",
              " 'Project_C2_Testing/003448.jpg',\n",
              " 'Project_C2_Testing/003550.jpg',\n",
              " 'Project_C2_Testing/003825.jpg',\n",
              " 'Project_C2_Testing/003880.jpg',\n",
              " 'Project_C2_Testing/004191.jpg',\n",
              " 'Project_C2_Testing/004539.jpg',\n",
              " 'Project_C2_Testing/004597.jpg',\n",
              " 'Project_C2_Testing/004898.jpg',\n",
              " 'Project_C2_Testing/004973.jpg',\n",
              " 'Project_C2_Testing/005127.jpg',\n",
              " 'Project_C2_Testing/005272.jpg',\n",
              " 'Project_C2_Testing/005592.jpg',\n",
              " 'Project_C2_Testing/005649.jpg',\n",
              " 'Project_C2_Testing/005829.jpg',\n",
              " 'Project_C2_Testing/005981.jpg',\n",
              " 'Project_C2_Testing/006029.jpg',\n",
              " 'Project_C2_Testing/006164.jpg',\n",
              " 'Project_C2_Testing/006253.jpg',\n",
              " 'Project_C2_Testing/006623.jpg',\n",
              " 'Project_C2_Testing/006828.jpg',\n",
              " 'Project_C2_Testing/007129.jpg',\n",
              " 'Project_C2_Testing/007662.jpg',\n",
              " 'Project_C2_Testing/007675.jpg',\n",
              " 'Project_C2_Testing/007749.jpg',\n",
              " 'Project_C2_Testing/007880.jpg',\n",
              " 'Project_C2_Testing/007921.jpg',\n",
              " 'Project_C2_Testing/007963.jpg',\n",
              " 'Project_C2_Testing/008059.jpg',\n",
              " 'Project_C2_Testing/008208.jpg',\n",
              " 'Project_C2_Testing/008487.jpg',\n",
              " 'Project_C2_Testing/008574.jpg',\n",
              " 'Project_C2_Testing/008628.jpg',\n",
              " 'Project_C2_Testing/008705.jpg',\n",
              " 'Project_C2_Testing/008827.jpg',\n",
              " 'Project_C2_Testing/008990.jpg',\n",
              " 'Project_C2_Testing/009135.jpg',\n",
              " 'Project_C2_Testing/009419.jpg',\n",
              " 'Project_C2_Testing/009496.jpg',\n",
              " 'Project_C2_Testing/009653.jpg',\n",
              " 'Project_C2_Testing/009903.jpg',\n",
              " 'Project_C2_Testing/010172.jpg',\n",
              " 'Project_C2_Testing/010516.jpg',\n",
              " 'Project_C2_Testing/010545.jpg',\n",
              " 'Project_C2_Testing/010595.jpg',\n",
              " 'Project_C2_Testing/010711.jpg',\n",
              " 'Project_C2_Testing/010806.jpg',\n",
              " 'Project_C2_Testing/010966.jpg',\n",
              " 'Project_C2_Testing/010999.jpg',\n",
              " 'Project_C2_Testing/011257.jpg',\n",
              " 'Project_C2_Testing/011305.jpg',\n",
              " 'Project_C2_Testing/011495.jpg',\n",
              " 'Project_C2_Testing/011553.jpg',\n",
              " 'Project_C2_Testing/011612.jpg',\n",
              " 'Project_C2_Testing/011674.jpg',\n",
              " 'Project_C2_Testing/012043.jpg',\n",
              " 'Project_C2_Testing/012068.jpg',\n",
              " 'Project_C2_Testing/012148.jpg',\n",
              " 'Project_C2_Testing/012263.jpg',\n",
              " 'Project_C2_Testing/012419.jpg',\n",
              " 'Project_C2_Testing/012543.jpg',\n",
              " 'Project_C2_Testing/012810.jpg',\n",
              " 'Project_C2_Testing/012847.jpg',\n",
              " 'Project_C2_Testing/012893.jpg',\n",
              " 'Project_C2_Testing/012923.jpg',\n",
              " 'Project_C2_Testing/012955.jpg',\n",
              " 'Project_C2_Testing/013010.jpg',\n",
              " 'Project_C2_Testing/013123.jpg',\n",
              " 'Project_C2_Testing/013436.jpg',\n",
              " 'Project_C2_Testing/013540.jpg',\n",
              " 'Project_C2_Testing/013591.jpg',\n",
              " 'Project_C2_Testing/013803.jpg',\n",
              " 'Project_C2_Testing/013818.jpg',\n",
              " 'Project_C2_Testing/013840.jpg',\n",
              " 'Project_C2_Testing/013936.jpg',\n",
              " 'Project_C2_Testing/014047.jpg',\n",
              " 'Project_C2_Testing/014143.jpg',\n",
              " 'Project_C2_Testing/014285.jpg',\n",
              " 'Project_C2_Testing/014614.jpg',\n",
              " 'Project_C2_Testing/014643.jpg',\n",
              " 'Project_C2_Testing/014761.jpg',\n",
              " 'Project_C2_Testing/014943.jpg',\n",
              " 'Project_C2_Testing/015229.jpg',\n",
              " 'Project_C2_Testing/015363.jpg',\n",
              " 'Project_C2_Testing/015503.jpg',\n",
              " 'Project_C2_Testing/015684.jpg',\n",
              " 'Project_C2_Testing/015986.jpg',\n",
              " 'Project_C2_Testing/016015.jpg',\n",
              " 'Project_C2_Testing/016127.jpg',\n",
              " 'Project_C2_Testing/016356.jpg',\n",
              " 'Project_C2_Testing/016682.jpg',\n",
              " 'Project_C2_Testing/016707.jpg',\n",
              " 'Project_C2_Testing/016733.jpg',\n",
              " 'Project_C2_Testing/016995.jpg',\n",
              " 'Project_C2_Testing/017095.jpg',\n",
              " 'Project_C2_Testing/017199.jpg',\n",
              " 'Project_C2_Testing/017213.jpg',\n",
              " 'Project_C2_Testing/017384.jpg',\n",
              " 'Project_C2_Testing/017545.jpg',\n",
              " 'Project_C2_Testing/017777.jpg',\n",
              " 'Project_C2_Testing/017780.jpg',\n",
              " 'Project_C2_Testing/017909.jpg',\n",
              " 'Project_C2_Testing/017923.jpg',\n",
              " 'Project_C2_Testing/017955.jpg',\n",
              " 'Project_C2_Testing/018036.jpg',\n",
              " 'Project_C2_Testing/018658.jpg',\n",
              " 'Project_C2_Testing/018945.jpg',\n",
              " 'Project_C2_Testing/018964.jpg',\n",
              " 'Project_C2_Testing/019159.jpg',\n",
              " 'Project_C2_Testing/019190.jpg']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc6CqQ1S2j1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y_pred = to_categorical(predicted_classes, dtype = 'int32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwZG3R2S8cPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "\n",
        "with open('labels.csv', 'wt') as output:\n",
        "    writer = csv.writer(output)\n",
        "    writer.writerows(y_pred)    \n",
        "output.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiOvx3Q48efY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "f = open('history_final.pckl', 'wb')\n",
        "pickle.dump(history.history, f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Pol4nYFGNiI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}